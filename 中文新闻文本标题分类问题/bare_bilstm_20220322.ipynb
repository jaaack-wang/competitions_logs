{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\n",
      "Collecting paddlepaddle\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/e4/42/3a91bff89038a9773b2df69610293e759ed6e1de6fe115503d1132805103/paddlepaddle-2.2.2-cp37-cp37m-manylinux1_x86_64.whl (108.4 MB)\n",
      "     |████████████████████████████████| 108.4 MB 3.2 MB/s            \n",
      "\u001b[?25hRequirement already satisfied: six in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlepaddle) (1.16.0)\n",
      "Requirement already satisfied: protobuf>=3.1.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlepaddle) (3.14.0)\n",
      "Requirement already satisfied: astor in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlepaddle) (0.8.1)\n",
      "Requirement already satisfied: decorator in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlepaddle) (4.4.0)\n",
      "Requirement already satisfied: requests>=2.20.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlepaddle) (2.22.0)\n",
      "Requirement already satisfied: Pillow in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlepaddle) (7.1.2)\n",
      "Requirement already satisfied: numpy>=1.13 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlepaddle) (1.16.4)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from requests>=2.20.0->paddlepaddle) (3.0.4)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from requests>=2.20.0->paddlepaddle) (2.8)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from requests>=2.20.0->paddlepaddle) (1.25.6)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from requests>=2.20.0->paddlepaddle) (2019.9.11)\n",
      "Installing collected packages: paddlepaddle\n",
      "Successfully installed paddlepaddle-2.2.2\n",
      "\u001b[33mWARNING: You are using pip version 21.3.1; however, version 22.0.4 is available.\n",
      "You should consider upgrading via the '/opt/conda/envs/python35-paddle120-env/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install paddlepaddle --upgrade"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 1. Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def load_dataset(fpath, test=False, num_row_to_skip=0):\n",
    "    data = open(fpath)\n",
    "    for _ in range(num_row_to_skip):\n",
    "        next(data)\n",
    "\n",
    "    out = []\n",
    "\n",
    "    if test:\n",
    "        for line in data:\n",
    "            out.append(line.strip())\n",
    "        \n",
    "        return out\n",
    "\n",
    "    idx_to_label = {}\n",
    "    for line in data:\n",
    "        line = line.strip().split('\\t')\n",
    "        if len(line) == 3:\n",
    "            idx, label, text = line\n",
    "            idx = int(idx)\n",
    "            idx_to_label[idx] = label\n",
    "            out.append([text, idx])\n",
    "    \n",
    "    return out, idx_to_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(752471,\n",
       " [['上证50ETF净申购突增', 0], ['交银施罗德保本基金将发行', 0]],\n",
       " {0: '财经',\n",
       "  1: '彩票',\n",
       "  2: '房产',\n",
       "  3: '股票',\n",
       "  4: '家居',\n",
       "  5: '教育',\n",
       "  6: '科技',\n",
       "  7: '社会',\n",
       "  8: '时尚',\n",
       "  9: '时政',\n",
       "  10: '体育',\n",
       "  11: '星座',\n",
       "  12: '游戏',\n",
       "  13: '娱乐'})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set, idx_to_label = load_dataset('./data/data12701/Train.txt')\n",
    "len(train_set), train_set[:2], idx_to_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# split the train_set into train and dev sets\n",
    "from random import shuffle, seed\n",
    "\n",
    "seed(43)\n",
    "shuffle(train_set)\n",
    "\n",
    "train_set, dev_set = train_set[:652471], train_set[652471: ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(83599, ['北京君太百货璀璨秋色 满100省353020元', '教育部：小学高年级将开始学习性知识'])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_set = load_dataset('./data/data12701/Test.txt', test=True)\n",
    "len(test_set), test_set[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 2. Transform text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from paddlenlp.datasets import MapDataset\n",
    "from paddle.io import BatchSampler, DataLoader\n",
    "from paddlenlp.data import Pad, Stack, Tuple\n",
    "from paddlenlp.data import Vocab\n",
    "\n",
    "\n",
    "class TextVectorizer:\n",
    "     \n",
    "    def __init__(self, tokenizer=None):\n",
    "        self.tokenize = tokenizer\n",
    "        self.vocab_to_idx = None\n",
    "        self._V = None\n",
    "    \n",
    "    def build_vocab(self, text):\n",
    "        tokens = list(map(self.tokenize, text))\n",
    "        self._V = Vocab.build_vocab(tokens, unk_token='[UNK]', pad_token='[PAD]')\n",
    "        self.vocab_to_idx = self._V.token_to_idx\n",
    "        \n",
    "    def text_encoder(self, text):\n",
    "        if isinstance(text, list):\n",
    "            return [self(t) for t in text]\n",
    "        \n",
    "        tks = self.tokenize(text)\n",
    "        out = [self.vocab_to_idx[tk] for tk in tks]\n",
    "        return out\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.vocab_to_idx)\n",
    "\n",
    "    def __getitem__(self, w):\n",
    "        return self.vocab_to_idx[w]\n",
    "    \n",
    "    def __call__(self, text):\n",
    "        if self.vocab_to_idx:\n",
    "            return self.text_encoder(text)\n",
    "        raise ValueError(\"No vocab is built!\")\n",
    "\n",
    "\n",
    "def example_converter(example, text_encoder, include_seq_len):\n",
    "    \n",
    "    text, label = example\n",
    "    encoded = text_encoder(text)\n",
    "    if include_seq_len:\n",
    "        text_len = len(encoded)\n",
    "        return encoded, text_len, label\n",
    "    return encoded, label\n",
    "\n",
    "\n",
    "def get_trans_fn(text_encoder, include_seq_len):\n",
    "    return lambda ex: example_converter(ex, text_encoder, include_seq_len)\n",
    "\n",
    "\n",
    "def get_batchify_fn(include_seq_len):\n",
    "    \n",
    "    if include_seq_len:\n",
    "        stack = [Stack(dtype=\"int64\")] * 2\n",
    "    else:\n",
    "        stack = [Stack(dtype=\"int64\")]\n",
    "    \n",
    "    batchify_fn = lambda samples, fn=Tuple(\n",
    "        Pad(axis=0, pad_val=0),  \n",
    "        *stack\n",
    "    ): fn(samples)\n",
    "    \n",
    "    return batchify_fn\n",
    "\n",
    "\n",
    "def create_dataloader(dataset, \n",
    "                      trans_fn, \n",
    "                      batchify_fn, \n",
    "                      test=False,\n",
    "                      batch_size=128, \n",
    "                      shuffle=True, \n",
    "                      sampler=BatchSampler):\n",
    "    \n",
    "    if test:\n",
    "        dataset = [[d, 0] for d in dataset]\n",
    "\n",
    "    if not isinstance(dataset, MapDataset):\n",
    "        dataset = MapDataset(dataset)\n",
    "        \n",
    "    dataset.map(trans_fn)\n",
    "    batch_sampler = sampler(dataset, \n",
    "                            shuffle=shuffle, \n",
    "                            batch_size=batch_size)\n",
    "    \n",
    "    dataloder = DataLoader(dataset, \n",
    "                           batch_sampler=batch_sampler, \n",
    "                           collate_fn=batchify_fn)\n",
    "    \n",
    "    return dataloder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of vocab (char): 5207\n"
     ]
    }
   ],
   "source": [
    "text = [t[0] for t in train_set]\n",
    "V = TextVectorizer(list)\n",
    "V.build_vocab(text)\n",
    "print(\"Number of vocab (char):\", len(V))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "include_seq_len = True; batch_size = 256\n",
    "trans_fn = get_trans_fn(V, include_seq_len=include_seq_len)\n",
    "batchify_fn = get_batchify_fn(include_seq_len=include_seq_len)\n",
    "train_loader = create_dataloader(train_set, trans_fn, batchify_fn, batch_size=batch_size)\n",
    "dev_loader = create_dataloader(dev_set, trans_fn, batchify_fn, batch_size=batch_size)\n",
    "test_loader = create_dataloader(test_set, trans_fn, batchify_fn, test=True,\n",
    "                                shuffle=False, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 3. Model building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import paddle, paddlenlp\n",
    "import paddle.nn as nn\n",
    "import paddle.nn.functional as F\n",
    "\n",
    "\n",
    "class LSTM(nn.Layer):\n",
    "    def __init__(self,\n",
    "                 vocab_size,\n",
    "                 num_classes,\n",
    "                 emb_dim=128,\n",
    "                 padding_idx=0,\n",
    "                 lstm_hidden_size=198,\n",
    "                 direction='forward',\n",
    "                 lstm_layers=1,\n",
    "                 dropout_rate=0.0,\n",
    "                 pooling_type=None,\n",
    "                 fc_hidden_size=96):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embedder = nn.Embedding(\n",
    "            num_embeddings=vocab_size,\n",
    "            embedding_dim=emb_dim,\n",
    "            padding_idx=padding_idx)\n",
    "\n",
    "        self.lstm_encoder = paddlenlp.seq2vec.LSTMEncoder(\n",
    "            emb_dim,\n",
    "            lstm_hidden_size,\n",
    "            num_layers=lstm_layers,\n",
    "            direction=direction,\n",
    "            dropout=dropout_rate,\n",
    "            pooling_type=pooling_type)\n",
    "\n",
    "        self.fc = nn.Linear(self.lstm_encoder.get_output_dim(), fc_hidden_size)\n",
    "        self.output_layer = nn.Linear(fc_hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, text, seq_len):\n",
    "        embedded_text = self.embedder(text)\n",
    "        text_repr = self.lstm_encoder(embedded_text, sequence_length=seq_len)\n",
    "        fc_out = paddle.tanh(self.fc(text_repr))\n",
    "        logits = self.output_layer(fc_out)\n",
    "        return logits\n",
    "\n",
    "\n",
    "def get_model(model):\n",
    "    model = paddle.Model(model)\n",
    "    optimizer = paddle.optimizer.Adam(\n",
    "    parameters=model.parameters(), learning_rate=5e-4)\n",
    "    criterion = paddle.nn.CrossEntropyLoss()\n",
    "    metric = paddle.metric.Accuracy()\n",
    "    model.prepare(optimizer, criterion, metric)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = LSTM(len(V), len(idx_to_label), direction='bidirectional')\n",
    "model = get_model(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 4. Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The loss value printed in the log is the current step, and the metric is the average value of previous steps.\n",
      "Epoch 1/10\n",
      "step  200/2549 - loss: 0.8721 - acc: 0.4932 - 470ms/step\n",
      "step  400/2549 - loss: 0.6373 - acc: 0.6327 - 472ms/step\n",
      "step  600/2549 - loss: 0.5097 - acc: 0.6966 - 475ms/step\n",
      "step  800/2549 - loss: 0.5186 - acc: 0.7331 - 474ms/step\n",
      "step 1000/2549 - loss: 0.4713 - acc: 0.7580 - 474ms/step\n",
      "step 1200/2549 - loss: 0.4068 - acc: 0.7759 - 474ms/step\n",
      "step 1400/2549 - loss: 0.3895 - acc: 0.7900 - 474ms/step\n",
      "step 1600/2549 - loss: 0.4493 - acc: 0.8010 - 473ms/step\n",
      "step 1800/2549 - loss: 0.3655 - acc: 0.8102 - 476ms/step\n",
      "step 2000/2549 - loss: 0.3071 - acc: 0.8175 - 478ms/step\n",
      "step 2200/2549 - loss: 0.2517 - acc: 0.8239 - 478ms/step\n",
      "step 2400/2549 - loss: 0.3058 - acc: 0.8293 - 480ms/step\n",
      "step 2549/2549 - loss: 0.2621 - acc: 0.8328 - 481ms/step\n",
      "Eval begin...\n",
      "step 200/391 - loss: 0.3705 - acc: 0.8881 - 180ms/step\n",
      "step 391/391 - loss: 0.2344 - acc: 0.8889 - 178ms/step\n",
      "Eval samples: 100000\n",
      "Epoch 2/10\n",
      "step  200/2549 - loss: 0.3212 - acc: 0.8989 - 508ms/step\n",
      "step  400/2549 - loss: 0.2938 - acc: 0.8984 - 507ms/step\n",
      "step  600/2549 - loss: 0.2405 - acc: 0.8984 - 503ms/step\n",
      "step  800/2549 - loss: 0.4194 - acc: 0.8986 - 502ms/step\n",
      "step 1000/2549 - loss: 0.4170 - acc: 0.8987 - 496ms/step\n",
      "step 1200/2549 - loss: 0.2695 - acc: 0.8990 - 492ms/step\n",
      "step 1400/2549 - loss: 0.3261 - acc: 0.8993 - 492ms/step\n",
      "step 1600/2549 - loss: 0.2191 - acc: 0.9001 - 499ms/step\n",
      "step 1800/2549 - loss: 0.2610 - acc: 0.9004 - 497ms/step\n",
      "step 2000/2549 - loss: 0.4258 - acc: 0.9010 - 499ms/step\n",
      "step 2200/2549 - loss: 0.3414 - acc: 0.9013 - 497ms/step\n",
      "step 2400/2549 - loss: 0.2649 - acc: 0.9018 - 496ms/step\n",
      "step 2549/2549 - loss: 0.1586 - acc: 0.9021 - 497ms/step\n",
      "Eval begin...\n",
      "step 200/391 - loss: 0.3374 - acc: 0.9056 - 173ms/step\n",
      "step 391/391 - loss: 0.2348 - acc: 0.9045 - 174ms/step\n",
      "Eval samples: 100000\n",
      "Epoch 3/10\n",
      "step  200/2549 - loss: 0.2566 - acc: 0.9150 - 510ms/step\n",
      "step  400/2549 - loss: 0.3587 - acc: 0.9153 - 518ms/step\n",
      "step  600/2549 - loss: 0.2330 - acc: 0.9145 - 506ms/step\n",
      "step  800/2549 - loss: 0.2778 - acc: 0.9144 - 500ms/step\n",
      "step 1000/2549 - loss: 0.2800 - acc: 0.9139 - 494ms/step\n",
      "step 1200/2549 - loss: 0.2073 - acc: 0.9139 - 490ms/step\n",
      "step 1400/2549 - loss: 0.2101 - acc: 0.9140 - 488ms/step\n",
      "step 1600/2549 - loss: 0.3369 - acc: 0.9138 - 487ms/step\n",
      "step 1800/2549 - loss: 0.3125 - acc: 0.9140 - 486ms/step\n",
      "step 2000/2549 - loss: 0.2952 - acc: 0.9141 - 485ms/step\n",
      "step 2200/2549 - loss: 0.2728 - acc: 0.9142 - 483ms/step\n",
      "step 2400/2549 - loss: 0.2019 - acc: 0.9143 - 482ms/step\n",
      "step 2549/2549 - loss: 0.2689 - acc: 0.9144 - 482ms/step\n",
      "Eval begin...\n",
      "step 200/391 - loss: 0.2385 - acc: 0.9119 - 176ms/step\n",
      "step 391/391 - loss: 0.1798 - acc: 0.9131 - 176ms/step\n",
      "Eval samples: 100000\n",
      "Epoch 4/10\n",
      "step  200/2549 - loss: 0.2053 - acc: 0.9255 - 471ms/step\n",
      "step  400/2549 - loss: 0.2583 - acc: 0.9249 - 472ms/step\n",
      "step  600/2549 - loss: 0.2976 - acc: 0.9244 - 473ms/step\n",
      "step  800/2549 - loss: 0.2166 - acc: 0.9240 - 486ms/step\n",
      "step 1000/2549 - loss: 0.2234 - acc: 0.9236 - 512ms/step\n",
      "step 1200/2549 - loss: 0.1993 - acc: 0.9233 - 506ms/step\n",
      "step 1400/2549 - loss: 0.2606 - acc: 0.9231 - 500ms/step\n",
      "step 1600/2549 - loss: 0.2051 - acc: 0.9231 - 493ms/step\n",
      "step 1800/2549 - loss: 0.2314 - acc: 0.9230 - 491ms/step\n",
      "step 2000/2549 - loss: 0.3317 - acc: 0.9231 - 489ms/step\n",
      "step 2200/2549 - loss: 0.2075 - acc: 0.9230 - 488ms/step\n",
      "step 2400/2549 - loss: 0.2651 - acc: 0.9229 - 492ms/step\n",
      "step 2549/2549 - loss: 0.2144 - acc: 0.9230 - 492ms/step\n",
      "Eval begin...\n",
      "step 200/391 - loss: 0.3264 - acc: 0.9185 - 176ms/step\n",
      "step 391/391 - loss: 0.2725 - acc: 0.9174 - 186ms/step\n",
      "Eval samples: 100000\n",
      "Epoch 5/10\n",
      "step  200/2549 - loss: 0.2354 - acc: 0.9327 - 472ms/step\n",
      "step  400/2549 - loss: 0.2002 - acc: 0.9321 - 472ms/step\n",
      "step  600/2549 - loss: 0.1779 - acc: 0.9317 - 468ms/step\n",
      "step  800/2549 - loss: 0.2190 - acc: 0.9315 - 469ms/step\n",
      "step 1000/2549 - loss: 0.2071 - acc: 0.9311 - 502ms/step\n",
      "step 1200/2549 - loss: 0.1582 - acc: 0.9307 - 506ms/step\n",
      "step 1400/2549 - loss: 0.1874 - acc: 0.9300 - 504ms/step\n",
      "step 1600/2549 - loss: 0.1457 - acc: 0.9300 - 500ms/step\n",
      "step 1800/2549 - loss: 0.1879 - acc: 0.9296 - 497ms/step\n",
      "step 2000/2549 - loss: 0.1989 - acc: 0.9298 - 495ms/step\n",
      "step 2200/2549 - loss: 0.1953 - acc: 0.9297 - 493ms/step\n",
      "step 2400/2549 - loss: 0.2006 - acc: 0.9296 - 492ms/step\n",
      "step 2549/2549 - loss: 0.2495 - acc: 0.9295 - 491ms/step\n",
      "Eval begin...\n",
      "step 200/391 - loss: 0.2280 - acc: 0.9221 - 177ms/step\n",
      "step 391/391 - loss: 0.2789 - acc: 0.9213 - 176ms/step\n",
      "Eval samples: 100000\n",
      "Epoch 6/10\n",
      "step  200/2549 - loss: 0.1515 - acc: 0.9382 - 471ms/step\n",
      "step  400/2549 - loss: 0.2019 - acc: 0.9387 - 473ms/step\n",
      "step  600/2549 - loss: 0.2293 - acc: 0.9379 - 477ms/step\n",
      "step  800/2549 - loss: 0.1246 - acc: 0.9372 - 474ms/step\n",
      "step 1000/2549 - loss: 0.1973 - acc: 0.9366 - 472ms/step\n",
      "step 1200/2549 - loss: 0.1380 - acc: 0.9364 - 469ms/step\n",
      "step 1400/2549 - loss: 0.1720 - acc: 0.9363 - 467ms/step\n",
      "step 1600/2549 - loss: 0.2142 - acc: 0.9359 - 465ms/step\n",
      "step 1800/2549 - loss: 0.2271 - acc: 0.9356 - 463ms/step\n",
      "step 2000/2549 - loss: 0.2415 - acc: 0.9353 - 463ms/step\n",
      "step 2200/2549 - loss: 0.1994 - acc: 0.9353 - 463ms/step\n",
      "step 2400/2549 - loss: 0.1493 - acc: 0.9351 - 464ms/step\n",
      "step 2549/2549 - loss: 0.2074 - acc: 0.9350 - 465ms/step\n",
      "Eval begin...\n",
      "step 200/391 - loss: 0.1930 - acc: 0.9219 - 174ms/step\n",
      "step 391/391 - loss: 0.1761 - acc: 0.9235 - 174ms/step\n",
      "Eval samples: 100000\n",
      "Epoch 7/10\n",
      "step  200/2549 - loss: 0.1931 - acc: 0.9429 - 466ms/step\n",
      "step  400/2549 - loss: 0.1746 - acc: 0.9425 - 464ms/step\n",
      "step  600/2549 - loss: 0.2234 - acc: 0.9427 - 466ms/step\n",
      "step  800/2549 - loss: 0.1181 - acc: 0.9421 - 466ms/step\n",
      "step 1000/2549 - loss: 0.1826 - acc: 0.9414 - 466ms/step\n",
      "step 1200/2549 - loss: 0.2032 - acc: 0.9408 - 467ms/step\n",
      "step 1400/2549 - loss: 0.1336 - acc: 0.9406 - 468ms/step\n",
      "step 1600/2549 - loss: 0.1472 - acc: 0.9405 - 468ms/step\n",
      "step 1800/2549 - loss: 0.1671 - acc: 0.9402 - 468ms/step\n",
      "step 2000/2549 - loss: 0.2095 - acc: 0.9400 - 468ms/step\n",
      "step 2200/2549 - loss: 0.2029 - acc: 0.9398 - 467ms/step\n",
      "step 2400/2549 - loss: 0.1490 - acc: 0.9396 - 466ms/step\n",
      "step 2549/2549 - loss: 0.1535 - acc: 0.9395 - 465ms/step\n",
      "Eval begin...\n",
      "step 200/391 - loss: 0.2009 - acc: 0.9244 - 173ms/step\n",
      "step 391/391 - loss: 0.2583 - acc: 0.9251 - 172ms/step\n",
      "Eval samples: 100000\n",
      "Epoch 8/10\n",
      "step  200/2549 - loss: 0.1869 - acc: 0.9481 - 477ms/step\n",
      "step  400/2549 - loss: 0.1386 - acc: 0.9471 - 476ms/step\n",
      "step  600/2549 - loss: 0.1404 - acc: 0.9466 - 474ms/step\n",
      "step  800/2549 - loss: 0.1356 - acc: 0.9465 - 471ms/step\n",
      "step 1000/2549 - loss: 0.1932 - acc: 0.9463 - 470ms/step\n",
      "step 1200/2549 - loss: 0.1250 - acc: 0.9462 - 470ms/step\n",
      "step 1400/2549 - loss: 0.1527 - acc: 0.9459 - 469ms/step\n",
      "step 1600/2549 - loss: 0.1778 - acc: 0.9454 - 468ms/step\n",
      "step 1800/2549 - loss: 0.2096 - acc: 0.9447 - 469ms/step\n",
      "step 2000/2549 - loss: 0.1746 - acc: 0.9444 - 470ms/step\n",
      "step 2200/2549 - loss: 0.2409 - acc: 0.9444 - 470ms/step\n",
      "step 2400/2549 - loss: 0.1782 - acc: 0.9441 - 470ms/step\n",
      "step 2549/2549 - loss: 0.0879 - acc: 0.9439 - 469ms/step\n",
      "Eval begin...\n",
      "step 200/391 - loss: 0.1298 - acc: 0.9273 - 174ms/step\n",
      "step 391/391 - loss: 0.2818 - acc: 0.9272 - 174ms/step\n",
      "Eval samples: 100000\n",
      "Epoch 9/10\n",
      "step  200/2549 - loss: 0.1688 - acc: 0.9517 - 474ms/step\n",
      "step  400/2549 - loss: 0.1311 - acc: 0.9508 - 471ms/step\n",
      "step  600/2549 - loss: 0.1840 - acc: 0.9503 - 469ms/step\n",
      "step  800/2549 - loss: 0.1748 - acc: 0.9499 - 471ms/step\n",
      "step 1000/2549 - loss: 0.1809 - acc: 0.9497 - 472ms/step\n",
      "step 1200/2549 - loss: 0.1159 - acc: 0.9494 - 471ms/step\n",
      "step 1400/2549 - loss: 0.1655 - acc: 0.9492 - 470ms/step\n",
      "step 1600/2549 - loss: 0.1719 - acc: 0.9489 - 470ms/step\n",
      "step 1800/2549 - loss: 0.1679 - acc: 0.9486 - 469ms/step\n",
      "step 2000/2549 - loss: 0.1543 - acc: 0.9486 - 473ms/step\n",
      "step 2200/2549 - loss: 0.0967 - acc: 0.9483 - 474ms/step\n",
      "step 2400/2549 - loss: 0.1871 - acc: 0.9481 - 473ms/step\n",
      "step 2549/2549 - loss: 0.1527 - acc: 0.9480 - 473ms/step\n",
      "Eval begin...\n",
      "step 200/391 - loss: 0.2338 - acc: 0.9267 - 179ms/step\n",
      "step 391/391 - loss: 0.2111 - acc: 0.9273 - 177ms/step\n",
      "Eval samples: 100000\n",
      "Epoch 9: Early stopping.\n"
     ]
    }
   ],
   "source": [
    "from paddle.callbacks import EarlyStopping\n",
    "\n",
    "earlystop = EarlyStopping(patience=3)\n",
    "\n",
    "model.fit(train_loader, dev_loader, epochs=10, verbose=2, log_freq=200, callbacks=[earlystop])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 5. Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predict begin...\n",
      "step 327/327 [==============================] - ETA: 1:02 - 192ms/st - ETA: 59s - 183ms/step - ETA: 58s - 181ms/ste - ETA: 57s - 179ms/ste - ETA: 56s - 179ms/ste - ETA: 55s - 176ms/ste - ETA: 55s - 177ms/ste - ETA: 54s - 174ms/ste - ETA: 54s - 177ms/ste - ETA: 54s - 178ms/ste - ETA: 54s - 177ms/ste - ETA: 53s - 176ms/ste - ETA: 53s - 178ms/ste - ETA: 53s - 179ms/ste - ETA: 52s - 178ms/ste - ETA: 52s - 178ms/ste - ETA: 51s - 177ms/ste - ETA: 51s - 176ms/ste - ETA: 51s - 177ms/ste - ETA: 50s - 176ms/ste - ETA: 50s - 176ms/ste - ETA: 49s - 176ms/ste - ETA: 49s - 177ms/ste - ETA: 49s - 176ms/ste - ETA: 48s - 176ms/ste - ETA: 48s - 175ms/ste - ETA: 47s - 175ms/ste - ETA: 47s - 175ms/ste - ETA: 46s - 175ms/ste - ETA: 46s - 174ms/ste - ETA: 46s - 174ms/ste - ETA: 45s - 174ms/ste - ETA: 45s - 174ms/ste - ETA: 44s - 174ms/ste - ETA: 44s - 173ms/ste - ETA: 44s - 173ms/ste - ETA: 43s - 173ms/ste - ETA: 43s - 173ms/ste - ETA: 42s - 172ms/ste - ETA: 42s - 172ms/ste - ETA: 42s - 172ms/ste - ETA: 41s - 172ms/ste - ETA: 41s - 172ms/ste - ETA: 41s - 173ms/ste - ETA: 40s - 173ms/ste - ETA: 40s - 173ms/ste - ETA: 40s - 172ms/ste - ETA: 39s - 172ms/ste - ETA: 39s - 172ms/ste - ETA: 39s - 172ms/ste - ETA: 38s - 172ms/ste - ETA: 38s - 172ms/ste - ETA: 37s - 172ms/ste - ETA: 37s - 172ms/ste - ETA: 37s - 172ms/ste - ETA: 36s - 172ms/ste - ETA: 36s - 172ms/ste - ETA: 36s - 172ms/ste - ETA: 35s - 172ms/ste - ETA: 35s - 172ms/ste - ETA: 35s - 171ms/ste - ETA: 34s - 171ms/ste - ETA: 34s - 171ms/ste - ETA: 34s - 171ms/ste - ETA: 33s - 171ms/ste - ETA: 33s - 171ms/ste - ETA: 33s - 171ms/ste - ETA: 32s - 172ms/ste - ETA: 32s - 172ms/ste - ETA: 32s - 172ms/ste - ETA: 31s - 171ms/ste - ETA: 31s - 171ms/ste - ETA: 30s - 171ms/ste - ETA: 30s - 171ms/ste - ETA: 30s - 172ms/ste - ETA: 30s - 173ms/ste - ETA: 29s - 173ms/ste - ETA: 29s - 173ms/ste - ETA: 29s - 173ms/ste - ETA: 28s - 173ms/ste - ETA: 28s - 173ms/ste - ETA: 28s - 173ms/ste - ETA: 27s - 173ms/ste - ETA: 27s - 173ms/ste - ETA: 27s - 174ms/ste - ETA: 26s - 174ms/ste - ETA: 26s - 173ms/ste - ETA: 26s - 174ms/ste - ETA: 25s - 174ms/ste - ETA: 25s - 174ms/ste - ETA: 25s - 174ms/ste - ETA: 24s - 174ms/ste - ETA: 24s - 174ms/ste - ETA: 24s - 174ms/ste - ETA: 23s - 174ms/ste - ETA: 23s - 174ms/ste - ETA: 23s - 174ms/ste - ETA: 22s - 174ms/ste - ETA: 22s - 174ms/ste - ETA: 22s - 174ms/ste - ETA: 21s - 174ms/ste - ETA: 21s - 174ms/ste - ETA: 20s - 174ms/ste - ETA: 20s - 174ms/ste - ETA: 20s - 173ms/ste - ETA: 19s - 173ms/ste - ETA: 19s - 173ms/ste - ETA: 19s - 174ms/ste - ETA: 18s - 174ms/ste - ETA: 18s - 173ms/ste - ETA: 18s - 173ms/ste - ETA: 17s - 173ms/ste - ETA: 17s - 173ms/ste - ETA: 17s - 173ms/ste - ETA: 16s - 173ms/ste - ETA: 16s - 173ms/ste - ETA: 16s - 173ms/ste - ETA: 15s - 173ms/ste - ETA: 15s - 173ms/ste - ETA: 15s - 173ms/ste - ETA: 14s - 173ms/ste - ETA: 14s - 173ms/ste - ETA: 13s - 173ms/ste - ETA: 13s - 173ms/ste - ETA: 13s - 173ms/ste - ETA: 12s - 173ms/ste - ETA: 12s - 173ms/ste - ETA: 12s - 173ms/ste - ETA: 11s - 172ms/ste - ETA: 11s - 172ms/ste - ETA: 11s - 172ms/ste - ETA: 10s - 172ms/ste - ETA: 10s - 172ms/ste - ETA: 10s - 172ms/ste - ETA: 9s - 172ms/ste - ETA: 9s - 172ms/st - ETA: 9s - 172ms/st - ETA: 8s - 172ms/st - ETA: 8s - 172ms/st - ETA: 8s - 172ms/st - ETA: 7s - 172ms/st - ETA: 7s - 172ms/st - ETA: 7s - 172ms/st - ETA: 6s - 172ms/st - ETA: 6s - 172ms/st - ETA: 6s - 172ms/st - ETA: 5s - 172ms/st - ETA: 5s - 172ms/st - ETA: 4s - 172ms/st - ETA: 4s - 172ms/st - ETA: 4s - 172ms/st - ETA: 3s - 172ms/st - ETA: 3s - 172ms/st - ETA: 3s - 172ms/st - ETA: 2s - 172ms/st - ETA: 2s - 172ms/st - ETA: 2s - 172ms/st - ETA: 1s - 172ms/st - ETA: 1s - 172ms/st - ETA: 1s - 172ms/st - ETA: 0s - 172ms/st - ETA: 0s - 172ms/st - ETA: 0s - 172ms/st - 172ms/step          \n",
      "Predict samples: 83599\n"
     ]
    }
   ],
   "source": [
    "predictions = []\n",
    "logits = model.predict(test_loader)\n",
    "for batch in logits[0]:\n",
    "    batch = paddle.to_tensor(batch)\n",
    "    probs = F.softmax(batch, axis=1)\n",
    "    preds = paddle.argmax(probs, axis=1).numpy().tolist()\n",
    "    predictions.extend(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('result.txt', 'w') as f:\n",
    "    f.write(idx_to_label[predictions[0]])\n",
    "    for p in predictions[1:]:\n",
    "        f.write('\\n' + idx_to_label[p])\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "updating: result.txt (deflated 89%)\n"
     ]
    }
   ],
   "source": [
    "!zip result.txt.zip result.txt"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "py35-paddle1.2.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
