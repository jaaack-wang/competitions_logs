{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\n",
      "Requirement already satisfied: paddlepaddle in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (2.2.2)\n",
      "Requirement already satisfied: protobuf>=3.1.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlepaddle) (3.14.0)\n",
      "Requirement already satisfied: requests>=2.20.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlepaddle) (2.22.0)\n",
      "Requirement already satisfied: six in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlepaddle) (1.16.0)\n",
      "Requirement already satisfied: astor in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlepaddle) (0.8.1)\n",
      "Requirement already satisfied: Pillow in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlepaddle) (7.1.2)\n",
      "Requirement already satisfied: decorator in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlepaddle) (4.4.0)\n",
      "Requirement already satisfied: numpy>=1.13 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlepaddle) (1.16.4)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from requests>=2.20.0->paddlepaddle) (1.25.6)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from requests>=2.20.0->paddlepaddle) (2.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from requests>=2.20.0->paddlepaddle) (2019.9.11)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from requests>=2.20.0->paddlepaddle) (3.0.4)\n",
      "\u001b[33mWARNING: You are using pip version 21.3.1; however, version 22.0.4 is available.\n",
      "You should consider upgrading via the '/opt/conda/envs/python35-paddle120-env/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install paddlepaddle --upgrade"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(fpath, test=False, num_row_to_skip=0):\n",
    "    data = open(fpath)\n",
    "    for _ in range(num_row_to_skip):\n",
    "        next(data)\n",
    "\n",
    "    out = []\n",
    "\n",
    "    if test:\n",
    "        for line in data:\n",
    "            out.append(line.strip())\n",
    "        \n",
    "        return out\n",
    "\n",
    "    idx_to_label = {}\n",
    "    for line in data:\n",
    "        line = line.strip().split('\\t')\n",
    "        if len(line) == 3:\n",
    "            idx, label, text = line\n",
    "            idx = int(idx)\n",
    "            idx_to_label[idx] = label\n",
    "            out.append([text, idx])\n",
    "    \n",
    "    return out, idx_to_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(752471,\n",
       " [['上证50ETF净申购突增', 0], ['交银施罗德保本基金将发行', 0]],\n",
       " {0: '财经',\n",
       "  1: '彩票',\n",
       "  2: '房产',\n",
       "  3: '股票',\n",
       "  4: '家居',\n",
       "  5: '教育',\n",
       "  6: '科技',\n",
       "  7: '社会',\n",
       "  8: '时尚',\n",
       "  9: '时政',\n",
       "  10: '体育',\n",
       "  11: '星座',\n",
       "  12: '游戏',\n",
       "  13: '娱乐'})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set, idx_to_label = load_dataset('./data/data12701/Train.txt')\n",
    "len(train_set), train_set[:2], idx_to_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the train_set into train and dev sets\n",
    "from random import shuffle, seed\n",
    "\n",
    "seed(43)\n",
    "shuffle(train_set)\n",
    "\n",
    "train_set, dev_set = train_set[:652471], train_set[652471: ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(83599, ['北京君太百货璀璨秋色 满100省353020元', '教育部：小学高年级将开始学习性知识'])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_set = load_dataset('./data/data12701/Test.txt', test=True)\n",
    "len(test_set), test_set[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Transform text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from paddlenlp.datasets import MapDataset\n",
    "from paddle.io import BatchSampler, DataLoader\n",
    "from paddlenlp.data import Pad, Stack, Tuple\n",
    "from paddlenlp.data import Vocab\n",
    "\n",
    "\n",
    "class TextVectorizer:\n",
    "     \n",
    "    def __init__(self, tokenizer=None):\n",
    "        self.tokenize = tokenizer\n",
    "        self.vocab_to_idx = None\n",
    "        self._V = None\n",
    "    \n",
    "    def build_vocab(self, text):\n",
    "        tokens = list(map(self.tokenize, text))\n",
    "        self._V = Vocab.build_vocab(tokens, unk_token='[UNK]', pad_token='[PAD]')\n",
    "        self.vocab_to_idx = self._V.token_to_idx\n",
    "        \n",
    "    def text_encoder(self, text):\n",
    "        if isinstance(text, list):\n",
    "            return [self(t) for t in text]\n",
    "        \n",
    "        tks = self.tokenize(text)\n",
    "        out = [self.vocab_to_idx[tk] for tk in tks]\n",
    "        return out\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.vocab_to_idx)\n",
    "\n",
    "    def __getitem__(self, w):\n",
    "        return self.vocab_to_idx[w]\n",
    "    \n",
    "    def __call__(self, text):\n",
    "        if self.vocab_to_idx:\n",
    "            return self.text_encoder(text)\n",
    "        raise ValueError(\"No vocab is built!\")\n",
    "\n",
    "\n",
    "def example_converter(example, text_encoder, include_seq_len):\n",
    "    \n",
    "    text, label = example\n",
    "    encoded = text_encoder(text)\n",
    "    if include_seq_len:\n",
    "        text_len = len(encoded)\n",
    "        return encoded, text_len, label\n",
    "    return encoded, label\n",
    "\n",
    "\n",
    "def get_trans_fn(text_encoder, include_seq_len):\n",
    "    return lambda ex: example_converter(ex, text_encoder, include_seq_len)\n",
    "\n",
    "\n",
    "def get_batchify_fn(include_seq_len):\n",
    "    \n",
    "    if include_seq_len:\n",
    "        stack = [Stack(dtype=\"int64\")] * 2\n",
    "    else:\n",
    "        stack = [Stack(dtype=\"int64\")]\n",
    "    \n",
    "    batchify_fn = lambda samples, fn=Tuple(\n",
    "        Pad(axis=0, pad_val=0),  \n",
    "        *stack\n",
    "    ): fn(samples)\n",
    "    \n",
    "    return batchify_fn\n",
    "\n",
    "\n",
    "def create_dataloader(dataset, \n",
    "                      trans_fn, \n",
    "                      batchify_fn, \n",
    "                      test=False,\n",
    "                      batch_size=128, \n",
    "                      shuffle=True, \n",
    "                      sampler=BatchSampler):\n",
    "    \n",
    "    if test:\n",
    "        dataset = [[d, 0] for d in dataset]\n",
    "\n",
    "    if not isinstance(dataset, MapDataset):\n",
    "        dataset = MapDataset(dataset)\n",
    "        \n",
    "    dataset.map(trans_fn)\n",
    "    batch_sampler = sampler(dataset, \n",
    "                            shuffle=shuffle, \n",
    "                            batch_size=batch_size)\n",
    "    \n",
    "    dataloder = DataLoader(dataset, \n",
    "                           batch_sampler=batch_sampler, \n",
    "                           collate_fn=batchify_fn)\n",
    "    \n",
    "    return dataloder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of vocab (char): 5207\n"
     ]
    }
   ],
   "source": [
    "text = [t[0] for t in train_set]\n",
    "V = TextVectorizer(list)\n",
    "V.build_vocab(text)\n",
    "print(\"Number of vocab (char):\", len(V))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "include_seq_len = False; batch_size = 256\n",
    "trans_fn = get_trans_fn(V, include_seq_len=include_seq_len)\n",
    "batchify_fn = get_batchify_fn(include_seq_len=include_seq_len)\n",
    "train_loader = create_dataloader(train_set, trans_fn, batchify_fn, batch_size=batch_size)\n",
    "dev_loader = create_dataloader(dev_set, trans_fn, batchify_fn, batch_size=batch_size)\n",
    "test_loader = create_dataloader(test_set, trans_fn, batchify_fn, test=True,\n",
    "                                shuffle=False, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import paddle \n",
    "import paddle.nn as nn\n",
    "import paddle.nn.functional as F\n",
    "\n",
    "\n",
    "class CNN(nn.Layer):\n",
    "\n",
    "    def __init__(self,\n",
    "                 vocab_size,\n",
    "                 output_dim,\n",
    "                 embedding_dim=100,\n",
    "                 padding_idx=0,\n",
    "                 num_filter=256,\n",
    "                 filter_sizes=(3,),\n",
    "                 hidden_dim=50,\n",
    "                 activation=nn.ReLU()):\n",
    "        \n",
    "        super().__init__()\n",
    "\n",
    "        self.embedding = nn.Embedding(\n",
    "            vocab_size, embedding_dim, padding_idx=padding_idx)\n",
    "        \n",
    "        self.convs = nn.LayerList([\n",
    "            nn.Conv1D(\n",
    "                in_channels=embedding_dim,\n",
    "                out_channels=num_filter,\n",
    "                kernel_size=fz\n",
    "            ) for fz in filter_sizes\n",
    "        ])\n",
    "        self.dense = nn.Linear(len(filter_sizes) * num_filter, hidden_dim)\n",
    "        self.activation = activation\n",
    "        self.dense_out = nn.Linear(hidden_dim, output_dim)\n",
    "    \n",
    "    def encoder(self, embd):\n",
    "        embd = embd.transpose((0,2,1))\n",
    "        conved = [self.activation(conv(embd)) for conv in self.convs]\n",
    "        max_pooled = [F.adaptive_max_pool1d(conv, output_size=1).squeeze(2) for conv in conved]\n",
    "        pooled_concat = paddle.concat(max_pooled, axis=1)\n",
    "        return pooled_concat\n",
    " \n",
    "    def forward(self, text_ids):\n",
    "        text_embd = self.embedding(text_ids)\n",
    "        encoded = self.encoder(text_embd)\n",
    "        hidden_out = self.activation(self.dense(encoded))\n",
    "        out_logits = self.dense_out(hidden_out)\n",
    "        return out_logits\n",
    "\n",
    "\n",
    "def get_model(model):\n",
    "    model = paddle.Model(model)\n",
    "    optimizer = paddle.optimizer.Adam(\n",
    "    parameters=model.parameters(), learning_rate=5e-4)\n",
    "    criterion = paddle.nn.CrossEntropyLoss()\n",
    "    metric = paddle.metric.Accuracy()\n",
    "    model.prepare(optimizer, criterion, metric)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CNN(len(V), len(idx_to_label))\n",
    "model = get_model(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The loss value printed in the log is the current step, and the metric is the average value of previous steps.\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/layers/utils.py:77: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated, and in 3.8 it will stop working\n",
      "  return (isinstance(seq, collections.Sequence) and\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step  200/2549 - loss: 0.8965 - acc: 0.4866 - 103ms/step\n",
      "step  400/2549 - loss: 0.6837 - acc: 0.6349 - 103ms/step\n",
      "step  600/2549 - loss: 0.4480 - acc: 0.7015 - 103ms/step\n",
      "step  800/2549 - loss: 0.5344 - acc: 0.7391 - 103ms/step\n",
      "step 1000/2549 - loss: 0.4914 - acc: 0.7645 - 102ms/step\n",
      "step 1200/2549 - loss: 0.5045 - acc: 0.7827 - 102ms/step\n",
      "step 1400/2549 - loss: 0.3949 - acc: 0.7969 - 102ms/step\n",
      "step 1600/2549 - loss: 0.3352 - acc: 0.8082 - 102ms/step\n",
      "step 1800/2549 - loss: 0.2884 - acc: 0.8172 - 102ms/step\n",
      "step 2000/2549 - loss: 0.2896 - acc: 0.8250 - 102ms/step\n",
      "step 2200/2549 - loss: 0.2584 - acc: 0.8315 - 101ms/step\n",
      "step 2400/2549 - loss: 0.2963 - acc: 0.8372 - 102ms/step\n",
      "step 2549/2549 - loss: 0.3044 - acc: 0.8409 - 102ms/step\n",
      "Eval begin...\n",
      "step 200/391 - loss: 0.3078 - acc: 0.9015 - 37ms/step\n",
      "step 391/391 - loss: 0.2728 - acc: 0.9019 - 36ms/step\n",
      "Eval samples: 100000\n",
      "Epoch 2/20\n",
      "step  200/2549 - loss: 0.3650 - acc: 0.9131 - 101ms/step\n",
      "step  400/2549 - loss: 0.1999 - acc: 0.9140 - 101ms/step\n",
      "step  600/2549 - loss: 0.2671 - acc: 0.9145 - 101ms/step\n",
      "step  800/2549 - loss: 0.2216 - acc: 0.9150 - 101ms/step\n",
      "step 1000/2549 - loss: 0.2666 - acc: 0.9151 - 101ms/step\n",
      "step 1200/2549 - loss: 0.2959 - acc: 0.9151 - 101ms/step\n",
      "step 1400/2549 - loss: 0.3249 - acc: 0.9156 - 101ms/step\n",
      "step 1600/2549 - loss: 0.2035 - acc: 0.9159 - 101ms/step\n",
      "step 1800/2549 - loss: 0.2436 - acc: 0.9162 - 101ms/step\n",
      "step 2000/2549 - loss: 0.2304 - acc: 0.9165 - 101ms/step\n",
      "step 2200/2549 - loss: 0.2089 - acc: 0.9170 - 101ms/step\n",
      "step 2400/2549 - loss: 0.2051 - acc: 0.9174 - 101ms/step\n",
      "step 2549/2549 - loss: 0.2612 - acc: 0.9177 - 101ms/step\n",
      "Eval begin...\n",
      "step 200/391 - loss: 0.2325 - acc: 0.9185 - 38ms/step\n",
      "step 391/391 - loss: 0.2079 - acc: 0.9178 - 37ms/step\n",
      "Eval samples: 100000\n",
      "Epoch 3/20\n",
      "step  200/2549 - loss: 0.1691 - acc: 0.9370 - 102ms/step\n",
      "step  400/2549 - loss: 0.2005 - acc: 0.9357 - 102ms/step\n",
      "step  600/2549 - loss: 0.3687 - acc: 0.9361 - 102ms/step\n",
      "step  800/2549 - loss: 0.1782 - acc: 0.9359 - 102ms/step\n",
      "step 1000/2549 - loss: 0.1285 - acc: 0.9356 - 102ms/step\n",
      "step 1200/2549 - loss: 0.2807 - acc: 0.9352 - 102ms/step\n",
      "step 1400/2549 - loss: 0.2761 - acc: 0.9350 - 102ms/step\n",
      "step 1600/2549 - loss: 0.2361 - acc: 0.9350 - 102ms/step\n",
      "step 1800/2549 - loss: 0.2453 - acc: 0.9348 - 102ms/step\n",
      "step 2000/2549 - loss: 0.2944 - acc: 0.9346 - 102ms/step\n",
      "step 2200/2549 - loss: 0.1771 - acc: 0.9347 - 102ms/step\n",
      "step 2400/2549 - loss: 0.1959 - acc: 0.9345 - 102ms/step\n",
      "step 2549/2549 - loss: 0.2472 - acc: 0.9345 - 102ms/step\n",
      "Eval begin...\n",
      "step 200/391 - loss: 0.2556 - acc: 0.9222 - 37ms/step\n",
      "step 391/391 - loss: 0.3140 - acc: 0.9221 - 37ms/step\n",
      "Eval samples: 100000\n",
      "Epoch 4/20\n",
      "step  200/2549 - loss: 0.1681 - acc: 0.9516 - 100ms/step\n",
      "step  400/2549 - loss: 0.1789 - acc: 0.9496 - 101ms/step\n",
      "step  600/2549 - loss: 0.2331 - acc: 0.9487 - 101ms/step\n",
      "step  800/2549 - loss: 0.1275 - acc: 0.9484 - 101ms/step\n",
      "step 1000/2549 - loss: 0.1632 - acc: 0.9475 - 101ms/step\n",
      "step 1200/2549 - loss: 0.1578 - acc: 0.9471 - 101ms/step\n",
      "step 1400/2549 - loss: 0.1289 - acc: 0.9466 - 101ms/step\n",
      "step 1600/2549 - loss: 0.1622 - acc: 0.9465 - 101ms/step\n",
      "step 1800/2549 - loss: 0.2141 - acc: 0.9463 - 101ms/step\n",
      "step 2000/2549 - loss: 0.2127 - acc: 0.9460 - 101ms/step\n",
      "step 2200/2549 - loss: 0.1610 - acc: 0.9458 - 101ms/step\n",
      "step 2400/2549 - loss: 0.2518 - acc: 0.9457 - 101ms/step\n",
      "step 2549/2549 - loss: 0.1695 - acc: 0.9455 - 101ms/step\n",
      "Eval begin...\n",
      "step 200/391 - loss: 0.2421 - acc: 0.9214 - 37ms/step\n",
      "step 391/391 - loss: 0.1918 - acc: 0.9237 - 37ms/step\n",
      "Eval samples: 100000\n",
      "Epoch 5/20\n",
      "step  200/2549 - loss: 0.1346 - acc: 0.9595 - 101ms/step\n",
      "step  400/2549 - loss: 0.2146 - acc: 0.9593 - 101ms/step\n",
      "step  600/2549 - loss: 0.1536 - acc: 0.9586 - 101ms/step\n",
      "step  800/2549 - loss: 0.1250 - acc: 0.9582 - 101ms/step\n",
      "step 1000/2549 - loss: 0.1083 - acc: 0.9579 - 101ms/step\n",
      "step 1200/2549 - loss: 0.1910 - acc: 0.9572 - 101ms/step\n",
      "step 1400/2549 - loss: 0.1611 - acc: 0.9565 - 102ms/step\n",
      "step 1600/2549 - loss: 0.0974 - acc: 0.9563 - 102ms/step\n",
      "step 1800/2549 - loss: 0.1742 - acc: 0.9558 - 102ms/step\n",
      "step 2000/2549 - loss: 0.1595 - acc: 0.9554 - 102ms/step\n",
      "step 2200/2549 - loss: 0.1904 - acc: 0.9550 - 102ms/step\n",
      "step 2400/2549 - loss: 0.0814 - acc: 0.9547 - 101ms/step\n",
      "step 2549/2549 - loss: 0.1482 - acc: 0.9545 - 101ms/step\n",
      "Eval begin...\n",
      "step 200/391 - loss: 0.2177 - acc: 0.9213 - 37ms/step\n",
      "step 391/391 - loss: 0.4259 - acc: 0.9227 - 37ms/step\n",
      "Eval samples: 100000\n",
      "Epoch 6/20\n",
      "step  200/2549 - loss: 0.0949 - acc: 0.9690 - 103ms/step\n",
      "step  400/2549 - loss: 0.0976 - acc: 0.9673 - 103ms/step\n",
      "step  600/2549 - loss: 0.1194 - acc: 0.9666 - 103ms/step\n",
      "step  800/2549 - loss: 0.1223 - acc: 0.9657 - 103ms/step\n",
      "step 1000/2549 - loss: 0.0987 - acc: 0.9653 - 103ms/step\n",
      "step 1200/2549 - loss: 0.1254 - acc: 0.9648 - 103ms/step\n",
      "step 1400/2549 - loss: 0.0841 - acc: 0.9643 - 102ms/step\n",
      "step 1600/2549 - loss: 0.1211 - acc: 0.9639 - 102ms/step\n",
      "step 1800/2549 - loss: 0.1478 - acc: 0.9634 - 102ms/step\n",
      "step 2000/2549 - loss: 0.0930 - acc: 0.9630 - 102ms/step\n",
      "step 2200/2549 - loss: 0.2074 - acc: 0.9624 - 102ms/step\n",
      "step 2400/2549 - loss: 0.1010 - acc: 0.9620 - 102ms/step\n",
      "step 2549/2549 - loss: 0.1444 - acc: 0.9618 - 102ms/step\n",
      "Eval begin...\n",
      "step 200/391 - loss: 0.3538 - acc: 0.9229 - 37ms/step\n",
      "step 391/391 - loss: 0.2169 - acc: 0.9233 - 36ms/step\n",
      "Eval samples: 100000\n",
      "Epoch 7/20\n",
      "step  200/2549 - loss: 0.0787 - acc: 0.9738 - 101ms/step\n",
      "step  400/2549 - loss: 0.1115 - acc: 0.9736 - 100ms/step\n",
      "step  600/2549 - loss: 0.1246 - acc: 0.9731 - 100ms/step\n",
      "step  800/2549 - loss: 0.1104 - acc: 0.9725 - 101ms/step\n",
      "step 1000/2549 - loss: 0.0855 - acc: 0.9720 - 101ms/step\n",
      "step 1200/2549 - loss: 0.0957 - acc: 0.9715 - 101ms/step\n",
      "step 1400/2549 - loss: 0.1087 - acc: 0.9708 - 101ms/step\n",
      "step 1600/2549 - loss: 0.1065 - acc: 0.9700 - 101ms/step\n",
      "step 1800/2549 - loss: 0.1363 - acc: 0.9696 - 100ms/step\n",
      "step 2000/2549 - loss: 0.1353 - acc: 0.9689 - 101ms/step\n",
      "step 2200/2549 - loss: 0.1095 - acc: 0.9684 - 101ms/step\n",
      "step 2400/2549 - loss: 0.0985 - acc: 0.9679 - 101ms/step\n",
      "step 2549/2549 - loss: 0.1841 - acc: 0.9676 - 101ms/step\n",
      "Eval begin...\n",
      "step 200/391 - loss: 0.3262 - acc: 0.9222 - 36ms/step\n",
      "step 391/391 - loss: 0.2430 - acc: 0.9216 - 36ms/step\n",
      "Eval samples: 100000\n",
      "Epoch 7: Early stopping.\n"
     ]
    }
   ],
   "source": [
    "from paddle.callbacks import EarlyStopping\n",
    "\n",
    "earlystop = EarlyStopping(patience=3)\n",
    "\n",
    "model.fit(train_loader, dev_loader, epochs=20, verbose=2, log_freq=200, callbacks=[earlystop])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predict begin...\n",
      "step 327/327 [==============================] - ETA: 14s - 44ms/st - ETA: 12s - 40ms/st - ETA: 12s - 39ms/st - ETA: 11s - 37ms/st - ETA: 11s - 37ms/st - ETA: 11s - 37ms/st - ETA: 11s - 37ms/st - ETA: 11s - 36ms/st - ETA: 11s - 37ms/st - ETA: 11s - 37ms/st - ETA: 11s - 37ms/st - ETA: 11s - 37ms/st - ETA: 11s - 37ms/st - ETA: 11s - 37ms/st - ETA: 10s - 37ms/st - ETA: 10s - 37ms/st - ETA: 10s - 37ms/st - ETA: 10s - 37ms/st - ETA: 10s - 37ms/st - ETA: 10s - 37ms/st - ETA: 10s - 36ms/st - ETA: 10s - 36ms/st - ETA: 10s - 37ms/st - ETA: 10s - 36ms/st - ETA: 10s - 36ms/st - ETA: 10s - 36ms/st - ETA: 9s - 36ms/step - ETA: 9s - 36ms/ste - ETA: 9s - 36ms/ste - ETA: 9s - 36ms/ste - ETA: 9s - 36ms/ste - ETA: 9s - 36ms/ste - ETA: 9s - 36ms/ste - ETA: 9s - 36ms/ste - ETA: 9s - 36ms/ste - ETA: 9s - 36ms/ste - ETA: 9s - 36ms/ste - ETA: 9s - 36ms/ste - ETA: 8s - 36ms/ste - ETA: 8s - 36ms/ste - ETA: 8s - 36ms/ste - ETA: 8s - 36ms/ste - ETA: 8s - 36ms/ste - ETA: 8s - 36ms/ste - ETA: 8s - 36ms/ste - ETA: 8s - 36ms/ste - ETA: 8s - 36ms/ste - ETA: 8s - 36ms/ste - ETA: 8s - 36ms/ste - ETA: 8s - 36ms/ste - ETA: 8s - 36ms/ste - ETA: 8s - 36ms/ste - ETA: 7s - 36ms/ste - ETA: 7s - 36ms/ste - ETA: 7s - 36ms/ste - ETA: 7s - 36ms/ste - ETA: 7s - 36ms/ste - ETA: 7s - 36ms/ste - ETA: 7s - 36ms/ste - ETA: 7s - 36ms/ste - ETA: 7s - 36ms/ste - ETA: 7s - 36ms/ste - ETA: 7s - 36ms/ste - ETA: 7s - 36ms/ste - ETA: 7s - 36ms/ste - ETA: 7s - 36ms/ste - ETA: 6s - 36ms/ste - ETA: 6s - 36ms/ste - ETA: 6s - 36ms/ste - ETA: 6s - 36ms/ste - ETA: 6s - 36ms/ste - ETA: 6s - 36ms/ste - ETA: 6s - 36ms/ste - ETA: 6s - 36ms/ste - ETA: 6s - 36ms/ste - ETA: 6s - 36ms/ste - ETA: 6s - 36ms/ste - ETA: 6s - 36ms/ste - ETA: 6s - 36ms/ste - ETA: 6s - 36ms/ste - ETA: 5s - 36ms/ste - ETA: 5s - 36ms/ste - ETA: 5s - 36ms/ste - ETA: 5s - 36ms/ste - ETA: 5s - 36ms/ste - ETA: 5s - 36ms/ste - ETA: 5s - 36ms/ste - ETA: 5s - 36ms/ste - ETA: 5s - 36ms/ste - ETA: 5s - 36ms/ste - ETA: 5s - 36ms/ste - ETA: 5s - 36ms/ste - ETA: 5s - 36ms/ste - ETA: 4s - 36ms/ste - ETA: 4s - 36ms/ste - ETA: 4s - 36ms/ste - ETA: 4s - 36ms/ste - ETA: 4s - 36ms/ste - ETA: 4s - 36ms/ste - ETA: 4s - 36ms/ste - ETA: 4s - 36ms/ste - ETA: 4s - 36ms/ste - ETA: 4s - 36ms/ste - ETA: 4s - 36ms/ste - ETA: 4s - 36ms/ste - ETA: 4s - 36ms/ste - ETA: 4s - 36ms/ste - ETA: 3s - 36ms/ste - ETA: 3s - 36ms/ste - ETA: 3s - 36ms/ste - ETA: 3s - 36ms/ste - ETA: 3s - 36ms/ste - ETA: 3s - 36ms/ste - ETA: 3s - 36ms/ste - ETA: 3s - 36ms/ste - ETA: 3s - 36ms/ste - ETA: 3s - 36ms/ste - ETA: 3s - 36ms/ste - ETA: 3s - 36ms/ste - ETA: 3s - 36ms/ste - ETA: 3s - 36ms/ste - ETA: 2s - 36ms/ste - ETA: 2s - 36ms/ste - ETA: 2s - 36ms/ste - ETA: 2s - 36ms/ste - ETA: 2s - 36ms/ste - ETA: 2s - 36ms/ste - ETA: 2s - 36ms/ste - ETA: 2s - 36ms/ste - ETA: 2s - 36ms/ste - ETA: 2s - 36ms/ste - ETA: 2s - 36ms/ste - ETA: 2s - 36ms/ste - ETA: 2s - 36ms/ste - ETA: 2s - 36ms/ste - ETA: 1s - 36ms/ste - ETA: 1s - 36ms/ste - ETA: 1s - 36ms/ste - ETA: 1s - 36ms/ste - ETA: 1s - 36ms/ste - ETA: 1s - 36ms/ste - ETA: 1s - 36ms/ste - ETA: 1s - 36ms/ste - ETA: 1s - 36ms/ste - ETA: 1s - 36ms/ste - ETA: 1s - 36ms/ste - ETA: 1s - 36ms/ste - ETA: 1s - 36ms/ste - ETA: 1s - 36ms/ste - ETA: 0s - 36ms/ste - ETA: 0s - 36ms/ste - ETA: 0s - 36ms/ste - ETA: 0s - 36ms/ste - ETA: 0s - 36ms/ste - ETA: 0s - 36ms/ste - ETA: 0s - 36ms/ste - ETA: 0s - 36ms/ste - ETA: 0s - 36ms/ste - ETA: 0s - 36ms/ste - ETA: 0s - 36ms/ste - ETA: 0s - 36ms/ste - ETA: 0s - 36ms/ste - ETA: 0s - 36ms/ste - 36ms/step          \n",
      "Predict samples: 83599\n"
     ]
    }
   ],
   "source": [
    "predictions = []\n",
    "logits = model.predict(test_loader)\n",
    "for batch in logits[0]:\n",
    "    batch = paddle.to_tensor(batch)\n",
    "    probs = F.softmax(batch, axis=1)\n",
    "    preds = paddle.argmax(probs, axis=1).numpy().tolist()\n",
    "    predictions.extend(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('result.txt', 'w') as f:\n",
    "    f.write(idx_to_label[predictions[0]])\n",
    "    for p in predictions[1:]:\n",
    "        f.write('\\n' + idx_to_label[p])\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "updating: result.txt (deflated 89%)\r\n"
     ]
    }
   ],
   "source": [
    "!zip result.txt.zip result.txt"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
