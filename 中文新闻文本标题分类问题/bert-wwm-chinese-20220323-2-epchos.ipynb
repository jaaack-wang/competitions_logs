{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2022-03-23T18:51:18.172876Z",
     "iopub.status.busy": "2022-03-23T18:51:18.172026Z",
     "iopub.status.idle": "2022-03-23T18:51:18.178695Z",
     "shell.execute_reply": "2022-03-23T18:51:18.177875Z",
     "shell.execute_reply.started": "2022-03-23T18:51:18.172836Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def load_dataset(fpath, test=False, num_row_to_skip=0):\n",
    "    data = open(fpath)\n",
    "    for _ in range(num_row_to_skip):\n",
    "        next(data)\n",
    "\n",
    "    out = []\n",
    "\n",
    "    if test:\n",
    "        for line in data:\n",
    "            out.append(line.strip())\n",
    "        \n",
    "        return out\n",
    "\n",
    "    idx_to_label = {}\n",
    "    for line in data:\n",
    "        line = line.strip().split('\\t')\n",
    "        if len(line) == 3:\n",
    "            idx, label, text = line\n",
    "            idx = int(idx)\n",
    "            idx_to_label[idx] = label\n",
    "            out.append([text, idx])\n",
    "    \n",
    "    return out, idx_to_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2022-03-23T18:51:18.181143Z",
     "iopub.status.busy": "2022-03-23T18:51:18.180665Z",
     "iopub.status.idle": "2022-03-23T18:51:19.870598Z",
     "shell.execute_reply": "2022-03-23T18:51:19.869966Z",
     "shell.execute_reply.started": "2022-03-23T18:51:18.181096Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(752471,\n",
       " [['上证50ETF净申购突增', 0], ['交银施罗德保本基金将发行', 0]],\n",
       " {0: '财经',\n",
       "  1: '彩票',\n",
       "  2: '房产',\n",
       "  3: '股票',\n",
       "  4: '家居',\n",
       "  5: '教育',\n",
       "  6: '科技',\n",
       "  7: '社会',\n",
       "  8: '时尚',\n",
       "  9: '时政',\n",
       "  10: '体育',\n",
       "  11: '星座',\n",
       "  12: '游戏',\n",
       "  13: '娱乐'})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set, idx_to_label = load_dataset('./data/data12701/Train.txt')\n",
    "len(train_set), train_set[:2], idx_to_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2022-03-23T18:51:19.872094Z",
     "iopub.status.busy": "2022-03-23T18:51:19.871554Z",
     "iopub.status.idle": "2022-03-23T18:51:20.717540Z",
     "shell.execute_reply": "2022-03-23T18:51:20.716777Z",
     "shell.execute_reply.started": "2022-03-23T18:51:19.872070Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# split the train_set into train and dev sets\n",
    "from random import shuffle, seed\n",
    "\n",
    "seed(43)\n",
    "shuffle(train_set)\n",
    "\n",
    "train_set, dev_set = train_set[:652471], train_set[652471: ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2022-03-23T18:51:20.719680Z",
     "iopub.status.busy": "2022-03-23T18:51:20.719333Z",
     "iopub.status.idle": "2022-03-23T18:51:20.765708Z",
     "shell.execute_reply": "2022-03-23T18:51:20.765128Z",
     "shell.execute_reply.started": "2022-03-23T18:51:20.719647Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(83599, ['北京君太百货璀璨秋色 满100省353020元', '教育部：小学高年级将开始学习性知识'])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_set = load_dataset('./data/data12701/Test.txt', test=True)\n",
    "len(test_set), test_set[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Transform text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2022-03-23T18:53:04.084947Z",
     "iopub.status.busy": "2022-03-23T18:53:04.084078Z",
     "iopub.status.idle": "2022-03-23T18:53:04.250128Z",
     "shell.execute_reply": "2022-03-23T18:53:04.249548Z",
     "shell.execute_reply.started": "2022-03-23T18:53:04.084910Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-03-24 02:53:04,091] [    INFO] - Downloading http://paddlenlp.bj.bcebos.com/models/transformers/bert/bert-wwm-chinese-vocab.txt and saved to /home/aistudio/.paddlenlp/models/bert-wwm-chinese\n",
      "[2022-03-24 02:53:04,094] [    INFO] - Downloading bert-wwm-chinese-vocab.txt from http://paddlenlp.bj.bcebos.com/models/transformers/bert/bert-wwm-chinese-vocab.txt\n",
      "100%|██████████| 107/107 [00:00<00:00, 3918.37it/s]\n"
     ]
    }
   ],
   "source": [
    "from paddlenlp.datasets import MapDataset\n",
    "from paddle.io import BatchSampler, DataLoader\n",
    "from paddlenlp.data import Pad, Stack, Tuple\n",
    "from paddlenlp import transformers\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "MODEL_NAME = \"bert-wwm-chinese\"\n",
    "tokenizer = transformers.BertTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "\n",
    "def example_converter(example, tokenizer, max_seq_length=128):\n",
    "    \n",
    "    text, label = example\n",
    "    encoded_inputs = tokenizer(text=text, max_seq_len=max_seq_length)\n",
    "    input_ids = encoded_inputs[\"input_ids\"]\n",
    "    token_type_ids = encoded_inputs[\"token_type_ids\"]\n",
    "    label = np.array([label], dtype=\"int64\")\n",
    "    return input_ids, token_type_ids, label\n",
    "\n",
    "\n",
    "def get_trans_fn(text_encoder, max_seq_length=128):\n",
    "    return lambda ex: example_converter(ex, text_encoder, max_seq_length)\n",
    "\n",
    "\n",
    "def get_batchify_fn(tokenizer=tokenizer):\n",
    "    \n",
    "    batchify_fn = lambda samples, fn=Tuple(\n",
    "        Pad(axis=0, pad_val=tokenizer.pad_token_id), \n",
    "        Pad(axis=0, pad_val=tokenizer.pad_token_type_id),\n",
    "        Stack(dtype=\"int64\")\n",
    "    ): fn(samples)\n",
    "    \n",
    "    return batchify_fn\n",
    "\n",
    "\n",
    "def create_dataloader(dataset, \n",
    "                      trans_fn, \n",
    "                      batchify_fn, \n",
    "                      test=False,\n",
    "                      batch_size=128, \n",
    "                      shuffle=True, \n",
    "                      sampler=BatchSampler):\n",
    "    \n",
    "    if test:\n",
    "        dataset = [[d, 0] for d in dataset]\n",
    "\n",
    "    if not isinstance(dataset, MapDataset):\n",
    "        dataset = MapDataset(dataset)\n",
    "        \n",
    "    dataset.map(trans_fn)\n",
    "    batch_sampler = sampler(dataset, \n",
    "                            shuffle=shuffle, \n",
    "                            batch_size=batch_size)\n",
    "    \n",
    "    dataloder = DataLoader(dataset, \n",
    "                           batch_sampler=batch_sampler, \n",
    "                           collate_fn=batchify_fn)\n",
    "    \n",
    "    return dataloder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2022-03-23T18:53:11.692305Z",
     "iopub.status.busy": "2022-03-23T18:53:11.691478Z",
     "iopub.status.idle": "2022-03-23T18:53:12.716958Z",
     "shell.execute_reply": "2022-03-23T18:53:12.716209Z",
     "shell.execute_reply.started": "2022-03-23T18:53:11.692269Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "max_seq_length = 32; batch_size = 256\n",
    "trans_fn = get_trans_fn(tokenizer, max_seq_length)\n",
    "batchify_fn = get_batchify_fn()\n",
    "train_loader = create_dataloader(train_set, trans_fn, batchify_fn, batch_size=batch_size)\n",
    "dev_loader = create_dataloader(dev_set, trans_fn, batchify_fn, batch_size=batch_size)\n",
    "test_loader = create_dataloader(test_set, trans_fn, batchify_fn, shuffle=False, test=True, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model preparing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2022-03-23T18:53:32.101530Z",
     "iopub.status.busy": "2022-03-23T18:53:32.100609Z",
     "iopub.status.idle": "2022-03-23T18:53:47.813731Z",
     "shell.execute_reply": "2022-03-23T18:53:47.812826Z",
     "shell.execute_reply.started": "2022-03-23T18:53:32.101492Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-03-24 02:53:32,108] [    INFO] - Downloading http://paddlenlp.bj.bcebos.com/models/transformers/bert/bert-wwm-chinese.pdparams and saved to /home/aistudio/.paddlenlp/models/bert-wwm-chinese\n",
      "[2022-03-24 02:53:32,110] [    INFO] - Downloading bert-wwm-chinese.pdparams from http://paddlenlp.bj.bcebos.com/models/transformers/bert/bert-wwm-chinese.pdparams\n",
      "100%|██████████| 399504/399504 [00:09<00:00, 43480.97it/s]\n",
      "W0324 02:53:41.383281  5080 device_context.cc:447] Please NOTE: device: 0, GPU Compute Capability: 7.0, Driver API Version: 10.1, Runtime API Version: 10.1\n",
      "W0324 02:53:41.388607  5080 device_context.cc:465] device: 0, cuDNN Version: 7.6.\n"
     ]
    }
   ],
   "source": [
    "import paddle \n",
    "from paddlenlp.transformers import LinearDecayWithWarmup\n",
    "\n",
    "\n",
    "model = transformers.BertForSequenceClassification.from_pretrained(MODEL_NAME, num_classes=len(idx_to_label))\n",
    "\n",
    "learning_rate = 5e-5; epochs = 2\n",
    "warmup_proportion = 0.1; weight_decay = 0.01\n",
    "num_training_steps = len(train_loader) * epochs\n",
    "lr_scheduler = LinearDecayWithWarmup(learning_rate, num_training_steps, warmup_proportion)\n",
    "optimizer = paddle.optimizer.AdamW(\n",
    "    learning_rate=lr_scheduler,\n",
    "    parameters=model.parameters(),\n",
    "    weight_decay=weight_decay,\n",
    "    apply_decay_param_fun=lambda x: x in [\n",
    "        p.name for n, p in model.named_parameters()\n",
    "        if not any(nd in n for nd in [\"bias\", \"norm\"])\n",
    "    ])\n",
    "\n",
    "criterion = paddle.nn.loss.CrossEntropyLoss()\n",
    "metric = paddle.metric.Accuracy()\n",
    "\n",
    "\n",
    "@paddle.no_grad()\n",
    "def evaluate(model, criterion, metric, data_loader):\n",
    "    \"\"\"\n",
    "    Given a dataset, it evals model and computes the metric.\n",
    "\n",
    "    Args:\n",
    "        model(obj:`paddle.nn.Layer`): A model to classify texts.\n",
    "        data_loader(obj:`paddle.io.DataLoader`): The dataset loader which generates batches.\n",
    "        criterion(obj:`paddle.nn.Layer`): It can compute the loss.\n",
    "        metric(obj:`paddle.metric.Metric`): The evaluation metric.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    metric.reset()\n",
    "    losses = []\n",
    "    for batch in data_loader:\n",
    "        input_ids, token_type_ids, labels = batch\n",
    "        logits = model(input_ids, token_type_ids)\n",
    "        loss = criterion(logits, labels)\n",
    "        losses.append(loss.numpy())\n",
    "        correct = metric.compute(logits, labels)\n",
    "        metric.update(correct)\n",
    "        accu = metric.accumulate()\n",
    "    print(\"eval loss: %.5f, accu: %.5f\" % (np.mean(losses), accu))\n",
    "    model.train()\n",
    "    metric.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2022-03-23T18:53:49.197877Z",
     "iopub.status.busy": "2022-03-23T18:53:49.197423Z",
     "iopub.status.idle": "2022-03-23T19:25:37.255811Z",
     "shell.execute_reply": "2022-03-23T19:25:37.255168Z",
     "shell.execute_reply.started": "2022-03-23T18:53:49.197840Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "global step 200, epoch: 1, batch: 200, loss: 0.42227, acc: 0.62645\n",
      "global step 400, epoch: 1, batch: 400, loss: 0.23299, acc: 0.76833\n",
      "global step 600, epoch: 1, batch: 600, loss: 0.23012, acc: 0.81885\n",
      "global step 800, epoch: 1, batch: 800, loss: 0.17873, acc: 0.84664\n",
      "global step 1000, epoch: 1, batch: 1000, loss: 0.16995, acc: 0.86386\n",
      "global step 1200, epoch: 1, batch: 1200, loss: 0.22252, acc: 0.87629\n",
      "global step 1400, epoch: 1, batch: 1400, loss: 0.23287, acc: 0.88533\n",
      "global step 1600, epoch: 1, batch: 1600, loss: 0.11192, acc: 0.89233\n",
      "global step 1800, epoch: 1, batch: 1800, loss: 0.17965, acc: 0.89795\n",
      "global step 2000, epoch: 1, batch: 2000, loss: 0.07994, acc: 0.90273\n",
      "global step 2200, epoch: 1, batch: 2200, loss: 0.15280, acc: 0.90680\n",
      "global step 2400, epoch: 1, batch: 2400, loss: 0.14953, acc: 0.91003\n",
      "eval loss: 0.15034, accu: 0.95150\n",
      "global step 2600, epoch: 2, batch: 51, loss: 0.10957, acc: 0.95833\n",
      "global step 2800, epoch: 2, batch: 251, loss: 0.13835, acc: 0.95893\n",
      "global step 3000, epoch: 2, batch: 451, loss: 0.11139, acc: 0.95934\n",
      "global step 3200, epoch: 2, batch: 651, loss: 0.11779, acc: 0.96004\n",
      "global step 3400, epoch: 2, batch: 851, loss: 0.06214, acc: 0.95999\n",
      "global step 3600, epoch: 2, batch: 1051, loss: 0.08893, acc: 0.96045\n",
      "global step 3800, epoch: 2, batch: 1251, loss: 0.13715, acc: 0.96069\n",
      "global step 4000, epoch: 2, batch: 1451, loss: 0.17174, acc: 0.96100\n",
      "global step 4200, epoch: 2, batch: 1651, loss: 0.13335, acc: 0.96116\n",
      "global step 4400, epoch: 2, batch: 1851, loss: 0.13277, acc: 0.96144\n",
      "global step 4600, epoch: 2, batch: 2051, loss: 0.13169, acc: 0.96169\n",
      "global step 4800, epoch: 2, batch: 2251, loss: 0.08436, acc: 0.96187\n",
      "global step 5000, epoch: 2, batch: 2451, loss: 0.09739, acc: 0.96205\n",
      "eval loss: 0.13078, accu: 0.95835\n"
     ]
    }
   ],
   "source": [
    "import paddle.nn.functional as F\n",
    "\n",
    "\n",
    "global_step = 0\n",
    "paddle.set_device(\"gpu\")\n",
    "for epoch in range(1, epochs + 1):\n",
    "    for step, batch in enumerate(train_loader, start=1):\n",
    "        input_ids, segment_ids, labels = batch\n",
    "        logits = model(input_ids, segment_ids)\n",
    "        loss = criterion(logits, labels)\n",
    "        probs = F.softmax(logits, axis=1)\n",
    "        correct = metric.compute(probs, labels)\n",
    "        metric.update(correct)\n",
    "        acc = metric.accumulate()\n",
    "\n",
    "        global_step += 1\n",
    "        if global_step % 200 == 0 :\n",
    "            print(\"global step %d, epoch: %d, batch: %d, loss: %.5f, acc: %.5f\" % (global_step, epoch, step, loss, acc))\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        optimizer.clear_grad()\n",
    "    evaluate(model, criterion, metric, dev_loader)\n",
    "\n",
    "\n",
    "# model.save_pretrained('/home/aistudio/checkpoint')\n",
    "# tokenizer.save_pretrained('/home/aistudio/checkpoint')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2022-03-23T19:25:37.281629Z",
     "iopub.status.busy": "2022-03-23T19:25:37.281403Z",
     "iopub.status.idle": "2022-03-23T19:26:27.315951Z",
     "shell.execute_reply": "2022-03-23T19:26:27.315253Z",
     "shell.execute_reply.started": "2022-03-23T19:25:37.281608Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "predictions = []\n",
    "\n",
    "for batch in test_loader:\n",
    "    input_ids, segment_ids, _ = batch\n",
    "    logits = model(input_ids, segment_ids)\n",
    "    probs = F.softmax(logits, axis=1)\n",
    "    preds = paddle.argmax(probs, axis=1).numpy().tolist()\n",
    "    predictions.extend(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2022-03-23T19:26:27.317577Z",
     "iopub.status.busy": "2022-03-23T19:26:27.317045Z",
     "iopub.status.idle": "2022-03-23T19:26:27.346673Z",
     "shell.execute_reply": "2022-03-23T19:26:27.346001Z",
     "shell.execute_reply.started": "2022-03-23T19:26:27.317540Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with open('result.txt', 'w') as f:\n",
    "    f.write(idx_to_label[predictions[0]])\n",
    "    for p in predictions[1:]:\n",
    "        f.write('\\n' + idx_to_label[p])\n",
    "    f.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "py35-paddle1.2.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
