{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2022-03-23T19:50:39.798054Z",
     "iopub.status.busy": "2022-03-23T19:50:39.797813Z",
     "iopub.status.idle": "2022-03-23T19:50:39.803931Z",
     "shell.execute_reply": "2022-03-23T19:50:39.803414Z",
     "shell.execute_reply.started": "2022-03-23T19:50:39.798028Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def load_dataset(fpath, test=False, num_row_to_skip=0):\n",
    "    data = open(fpath)\n",
    "    for _ in range(num_row_to_skip):\n",
    "        next(data)\n",
    "\n",
    "    out = []\n",
    "\n",
    "    if test:\n",
    "        for line in data:\n",
    "            out.append(line.strip())\n",
    "        \n",
    "        return out\n",
    "\n",
    "    idx_to_label = {}\n",
    "    for line in data:\n",
    "        line = line.strip().split('\\t')\n",
    "        if len(line) == 3:\n",
    "            idx, label, text = line\n",
    "            idx = int(idx)\n",
    "            idx_to_label[idx] = label\n",
    "            out.append([text, idx])\n",
    "    \n",
    "    return out, idx_to_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2022-03-23T19:50:39.805385Z",
     "iopub.status.busy": "2022-03-23T19:50:39.805014Z",
     "iopub.status.idle": "2022-03-23T19:50:41.537628Z",
     "shell.execute_reply": "2022-03-23T19:50:41.536771Z",
     "shell.execute_reply.started": "2022-03-23T19:50:39.805362Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(752471,\n",
       " [['上证50ETF净申购突增', 0], ['交银施罗德保本基金将发行', 0]],\n",
       " {0: '财经',\n",
       "  1: '彩票',\n",
       "  2: '房产',\n",
       "  3: '股票',\n",
       "  4: '家居',\n",
       "  5: '教育',\n",
       "  6: '科技',\n",
       "  7: '社会',\n",
       "  8: '时尚',\n",
       "  9: '时政',\n",
       "  10: '体育',\n",
       "  11: '星座',\n",
       "  12: '游戏',\n",
       "  13: '娱乐'})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set, idx_to_label = load_dataset('./data/data12701/Train.txt')\n",
    "len(train_set), train_set[:2], idx_to_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2022-03-23T19:50:41.540280Z",
     "iopub.status.busy": "2022-03-23T19:50:41.539708Z",
     "iopub.status.idle": "2022-03-23T19:50:42.362937Z",
     "shell.execute_reply": "2022-03-23T19:50:42.362215Z",
     "shell.execute_reply.started": "2022-03-23T19:50:41.540222Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# split the train_set into train and dev sets\n",
    "from random import shuffle, seed\n",
    "\n",
    "seed(43)\n",
    "shuffle(train_set)\n",
    "\n",
    "train_set, dev_set = train_set[:652471], train_set[652471: ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2022-03-23T19:50:42.364916Z",
     "iopub.status.busy": "2022-03-23T19:50:42.364265Z",
     "iopub.status.idle": "2022-03-23T19:50:42.413439Z",
     "shell.execute_reply": "2022-03-23T19:50:42.412617Z",
     "shell.execute_reply.started": "2022-03-23T19:50:42.364868Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(83599, ['北京君太百货璀璨秋色 满100省353020元', '教育部：小学高年级将开始学习性知识'])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_set = load_dataset('./data/data12701/Test.txt', test=True)\n",
    "len(test_set), test_set[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Transform text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2022-03-23T19:50:42.415601Z",
     "iopub.status.busy": "2022-03-23T19:50:42.415203Z",
     "iopub.status.idle": "2022-03-23T19:50:44.340338Z",
     "shell.execute_reply": "2022-03-23T19:50:44.339661Z",
     "shell.execute_reply.started": "2022-03-23T19:50:42.415560Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-03-24 03:50:44,321] [    INFO] - Already cached /home/aistudio/.paddlenlp/models/bert-wwm-chinese/bert-wwm-chinese-vocab.txt\n"
     ]
    }
   ],
   "source": [
    "from paddlenlp.datasets import MapDataset\n",
    "from paddle.io import BatchSampler, DataLoader\n",
    "from paddlenlp.data import Pad, Stack, Tuple\n",
    "from paddlenlp.transformers import BertForSequenceClassification as SeqClfModel\n",
    "from paddlenlp.transformers import BertTokenizer as PTMTokenizer\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "MODEL_NAME = \"bert-wwm-chinese\"\n",
    "tokenizer = PTMTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "\n",
    "def example_converter(example, tokenizer, max_seq_length=128):\n",
    "    \n",
    "    text, label = example\n",
    "    encoded_inputs = tokenizer(text=text, max_seq_len=max_seq_length)\n",
    "    input_ids = encoded_inputs[\"input_ids\"]\n",
    "    token_type_ids = encoded_inputs[\"token_type_ids\"]\n",
    "    label = np.array([label], dtype=\"int64\")\n",
    "    return input_ids, token_type_ids, label\n",
    "\n",
    "\n",
    "def get_trans_fn(text_encoder, max_seq_length=128):\n",
    "    return lambda ex: example_converter(ex, text_encoder, max_seq_length)\n",
    "\n",
    "\n",
    "def get_batchify_fn(tokenizer=tokenizer):\n",
    "    \n",
    "    batchify_fn = lambda samples, fn=Tuple(\n",
    "        Pad(axis=0, pad_val=tokenizer.pad_token_id), \n",
    "        Pad(axis=0, pad_val=tokenizer.pad_token_type_id),\n",
    "        Stack(dtype=\"int64\")\n",
    "    ): fn(samples)\n",
    "    \n",
    "    return batchify_fn\n",
    "\n",
    "\n",
    "def create_dataloader(dataset, \n",
    "                      trans_fn, \n",
    "                      batchify_fn, \n",
    "                      test=False,\n",
    "                      batch_size=128, \n",
    "                      shuffle=True, \n",
    "                      sampler=BatchSampler):\n",
    "    \n",
    "    if test:\n",
    "        dataset = [[d, 0] for d in dataset]\n",
    "\n",
    "    if not isinstance(dataset, MapDataset):\n",
    "        dataset = MapDataset(dataset)\n",
    "        \n",
    "    dataset.map(trans_fn)\n",
    "    batch_sampler = sampler(dataset, \n",
    "                            shuffle=shuffle, \n",
    "                            batch_size=batch_size)\n",
    "    \n",
    "    dataloder = DataLoader(dataset, \n",
    "                           batch_sampler=batch_sampler, \n",
    "                           collate_fn=batchify_fn)\n",
    "    \n",
    "    return dataloder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2022-03-23T19:50:44.342086Z",
     "iopub.status.busy": "2022-03-23T19:50:44.341576Z",
     "iopub.status.idle": "2022-03-23T19:50:45.386046Z",
     "shell.execute_reply": "2022-03-23T19:50:45.385129Z",
     "shell.execute_reply.started": "2022-03-23T19:50:44.342056Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "max_seq_length = 16; batch_size = 512\n",
    "trans_fn = get_trans_fn(tokenizer, max_seq_length)\n",
    "batchify_fn = get_batchify_fn()\n",
    "train_loader = create_dataloader(train_set, trans_fn, batchify_fn, batch_size=batch_size)\n",
    "dev_loader = create_dataloader(dev_set, trans_fn, batchify_fn, batch_size=batch_size)\n",
    "test_loader = create_dataloader(test_set, trans_fn, batchify_fn, shuffle=False, test=True, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model preparing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2022-03-23T19:50:45.388565Z",
     "iopub.status.busy": "2022-03-23T19:50:45.388057Z",
     "iopub.status.idle": "2022-03-23T19:50:52.213725Z",
     "shell.execute_reply": "2022-03-23T19:50:52.213014Z",
     "shell.execute_reply.started": "2022-03-23T19:50:45.388511Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-03-24 03:50:45,396] [    INFO] - Already cached /home/aistudio/.paddlenlp/models/bert-wwm-chinese/bert-wwm-chinese.pdparams\n",
      "W0324 03:50:45.399821  1369 device_context.cc:447] Please NOTE: device: 0, GPU Compute Capability: 7.0, Driver API Version: 10.1, Runtime API Version: 10.1\n",
      "W0324 03:50:45.405444  1369 device_context.cc:465] device: 0, cuDNN Version: 7.6.\n"
     ]
    }
   ],
   "source": [
    "import paddle \n",
    "from paddlenlp.transformers import LinearDecayWithWarmup\n",
    "\n",
    "\n",
    "model = SeqClfModel.from_pretrained(MODEL_NAME, num_classes=len(idx_to_label))\n",
    "\n",
    "learning_rate = 5e-5; epochs = 5\n",
    "warmup_proportion = 0.1; weight_decay = 0.01\n",
    "num_training_steps = len(train_loader) * epochs\n",
    "lr_scheduler = LinearDecayWithWarmup(learning_rate, num_training_steps, warmup_proportion)\n",
    "optimizer = paddle.optimizer.AdamW(\n",
    "    learning_rate=lr_scheduler,\n",
    "    parameters=model.parameters(),\n",
    "    weight_decay=weight_decay,\n",
    "    apply_decay_param_fun=lambda x: x in [\n",
    "        p.name for n, p in model.named_parameters()\n",
    "        if not any(nd in n for nd in [\"bias\", \"norm\"])\n",
    "    ])\n",
    "\n",
    "criterion = paddle.nn.loss.CrossEntropyLoss()\n",
    "metric = paddle.metric.Accuracy()\n",
    "\n",
    "\n",
    "@paddle.no_grad()\n",
    "def evaluate(model, criterion, metric, data_loader):\n",
    "    \"\"\"\n",
    "    Given a dataset, it evals model and computes the metric.\n",
    "\n",
    "    Args:\n",
    "        model(obj:`paddle.nn.Layer`): A model to classify texts.\n",
    "        data_loader(obj:`paddle.io.DataLoader`): The dataset loader which generates batches.\n",
    "        criterion(obj:`paddle.nn.Layer`): It can compute the loss.\n",
    "        metric(obj:`paddle.metric.Metric`): The evaluation metric.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    metric.reset()\n",
    "    losses = []\n",
    "    for batch in data_loader:\n",
    "        input_ids, token_type_ids, labels = batch\n",
    "        logits = model(input_ids, token_type_ids)\n",
    "        loss = criterion(logits, labels)\n",
    "        losses.append(loss.numpy())\n",
    "        correct = metric.compute(logits, labels)\n",
    "        metric.update(correct)\n",
    "        accu = metric.accumulate()\n",
    "    print(\"eval loss: %.5f, accu: %.5f\" % (np.mean(losses), accu))\n",
    "    model.train()\n",
    "    metric.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2022-03-23T19:50:52.215600Z",
     "iopub.status.busy": "2022-03-23T19:50:52.215029Z",
     "iopub.status.idle": "2022-03-23T20:37:09.021499Z",
     "shell.execute_reply": "2022-03-23T20:37:09.020546Z",
     "shell.execute_reply.started": "2022-03-23T19:50:52.215570Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "global step 100, epoch: 1, batch: 100, loss: 1.14961, acc: 0.37242\n",
      "global step 200, epoch: 1, batch: 200, loss: 0.49208, acc: 0.60509\n",
      "global step 300, epoch: 1, batch: 300, loss: 0.32710, acc: 0.69915\n",
      "global step 400, epoch: 1, batch: 400, loss: 0.35039, acc: 0.74960\n",
      "global step 500, epoch: 1, batch: 500, loss: 0.30017, acc: 0.78177\n",
      "global step 600, epoch: 1, batch: 600, loss: 0.31458, acc: 0.80407\n",
      "global step 700, epoch: 1, batch: 700, loss: 0.21146, acc: 0.82052\n",
      "global step 800, epoch: 1, batch: 800, loss: 0.28852, acc: 0.83325\n",
      "global step 900, epoch: 1, batch: 900, loss: 0.31408, acc: 0.84324\n",
      "global step 1000, epoch: 1, batch: 1000, loss: 0.20978, acc: 0.85166\n",
      "global step 1100, epoch: 1, batch: 1100, loss: 0.20419, acc: 0.85862\n",
      "global step 1200, epoch: 1, batch: 1200, loss: 0.15890, acc: 0.86450\n",
      "eval loss: 0.20204, accu: 0.93618\n",
      "global step 1300, epoch: 2, batch: 25, loss: 0.14195, acc: 0.94328\n",
      "global step 1400, epoch: 2, batch: 125, loss: 0.18548, acc: 0.94320\n",
      "global step 1500, epoch: 2, batch: 225, loss: 0.14236, acc: 0.94305\n",
      "global step 1600, epoch: 2, batch: 325, loss: 0.14284, acc: 0.94302\n",
      "global step 1700, epoch: 2, batch: 425, loss: 0.18607, acc: 0.94254\n",
      "global step 1800, epoch: 2, batch: 525, loss: 0.17265, acc: 0.94314\n",
      "global step 1900, epoch: 2, batch: 625, loss: 0.24458, acc: 0.94325\n",
      "global step 2000, epoch: 2, batch: 725, loss: 0.13982, acc: 0.94356\n",
      "global step 2100, epoch: 2, batch: 825, loss: 0.16906, acc: 0.94378\n",
      "global step 2200, epoch: 2, batch: 925, loss: 0.18921, acc: 0.94407\n",
      "global step 2300, epoch: 2, batch: 1025, loss: 0.19881, acc: 0.94420\n",
      "global step 2400, epoch: 2, batch: 1125, loss: 0.15593, acc: 0.94453\n",
      "global step 2500, epoch: 2, batch: 1225, loss: 0.15159, acc: 0.94468\n",
      "eval loss: 0.17495, accu: 0.94481\n",
      "global step 2600, epoch: 3, batch: 50, loss: 0.08911, acc: 0.96199\n",
      "global step 2700, epoch: 3, batch: 150, loss: 0.12177, acc: 0.96069\n",
      "global step 2800, epoch: 3, batch: 250, loss: 0.10884, acc: 0.96042\n",
      "global step 2900, epoch: 3, batch: 350, loss: 0.09404, acc: 0.96028\n",
      "global step 3000, epoch: 3, batch: 450, loss: 0.13713, acc: 0.96000\n",
      "global step 3100, epoch: 3, batch: 550, loss: 0.15221, acc: 0.96008\n",
      "global step 3200, epoch: 3, batch: 650, loss: 0.09981, acc: 0.96009\n",
      "global step 3300, epoch: 3, batch: 750, loss: 0.11462, acc: 0.95999\n",
      "global step 3400, epoch: 3, batch: 850, loss: 0.11731, acc: 0.95984\n",
      "global step 3500, epoch: 3, batch: 950, loss: 0.12836, acc: 0.95972\n",
      "global step 3600, epoch: 3, batch: 1050, loss: 0.17218, acc: 0.95978\n",
      "global step 3700, epoch: 3, batch: 1150, loss: 0.09288, acc: 0.95980\n",
      "global step 3800, epoch: 3, batch: 1250, loss: 0.08262, acc: 0.95982\n",
      "eval loss: 0.17384, accu: 0.94670\n",
      "global step 3900, epoch: 4, batch: 75, loss: 0.06207, acc: 0.97148\n",
      "global step 4000, epoch: 4, batch: 175, loss: 0.07004, acc: 0.97142\n",
      "global step 4100, epoch: 4, batch: 275, loss: 0.07718, acc: 0.97138\n",
      "global step 4200, epoch: 4, batch: 375, loss: 0.12309, acc: 0.97090\n",
      "global step 4300, epoch: 4, batch: 475, loss: 0.07928, acc: 0.97108\n",
      "global step 4400, epoch: 4, batch: 575, loss: 0.08329, acc: 0.97094\n",
      "global step 4500, epoch: 4, batch: 675, loss: 0.07245, acc: 0.97104\n",
      "global step 4600, epoch: 4, batch: 775, loss: 0.06346, acc: 0.97098\n",
      "global step 4700, epoch: 4, batch: 875, loss: 0.10350, acc: 0.97106\n",
      "global step 4800, epoch: 4, batch: 975, loss: 0.09613, acc: 0.97096\n",
      "global step 4900, epoch: 4, batch: 1075, loss: 0.11594, acc: 0.97090\n",
      "global step 5000, epoch: 4, batch: 1175, loss: 0.12877, acc: 0.97083\n",
      "global step 5100, epoch: 4, batch: 1275, loss: 0.05885, acc: 0.97090\n",
      "eval loss: 0.17843, accu: 0.94781\n",
      "global step 5200, epoch: 5, batch: 100, loss: 0.08335, acc: 0.97920\n",
      "global step 5300, epoch: 5, batch: 200, loss: 0.06204, acc: 0.97961\n",
      "global step 5400, epoch: 5, batch: 300, loss: 0.05101, acc: 0.97929\n",
      "global step 5500, epoch: 5, batch: 400, loss: 0.05549, acc: 0.97917\n",
      "global step 5600, epoch: 5, batch: 500, loss: 0.07221, acc: 0.97921\n",
      "global step 5700, epoch: 5, batch: 600, loss: 0.04945, acc: 0.97924\n",
      "global step 5800, epoch: 5, batch: 700, loss: 0.06306, acc: 0.97927\n",
      "global step 5900, epoch: 5, batch: 800, loss: 0.06025, acc: 0.97920\n",
      "global step 6000, epoch: 5, batch: 900, loss: 0.04637, acc: 0.97919\n",
      "global step 6100, epoch: 5, batch: 1000, loss: 0.08122, acc: 0.97925\n",
      "global step 6200, epoch: 5, batch: 1100, loss: 0.06076, acc: 0.97923\n",
      "global step 6300, epoch: 5, batch: 1200, loss: 0.06735, acc: 0.97936\n",
      "eval loss: 0.18798, accu: 0.94755\n"
     ]
    }
   ],
   "source": [
    "import paddle.nn.functional as F\n",
    "\n",
    "\n",
    "global_step = 0\n",
    "paddle.set_device(\"gpu\")\n",
    "for epoch in range(1, epochs + 1):\n",
    "    for step, batch in enumerate(train_loader, start=1):\n",
    "        input_ids, segment_ids, labels = batch\n",
    "        logits = model(input_ids, segment_ids)\n",
    "        loss = criterion(logits, labels)\n",
    "        probs = F.softmax(logits, axis=1)\n",
    "        correct = metric.compute(probs, labels)\n",
    "        metric.update(correct)\n",
    "        acc = metric.accumulate()\n",
    "\n",
    "        global_step += 1\n",
    "        if global_step % 100 == 0 :\n",
    "            print(\"global step %d, epoch: %d, batch: %d, loss: %.5f, acc: %.5f\" % (global_step, epoch, step, loss, acc))\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        optimizer.clear_grad()\n",
    "    evaluate(model, criterion, metric, dev_loader)\n",
    "\n",
    "\n",
    "# model.save_pretrained('/home/aistudio/checkpoint')\n",
    "# tokenizer.save_pretrained('/home/aistudio/checkpoint')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2022-03-23T20:37:09.024035Z",
     "iopub.status.busy": "2022-03-23T20:37:09.023101Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "predictions = []\n",
    "\n",
    "for batch in test_loader:\n",
    "    input_ids, segment_ids, _ = batch\n",
    "    logits = model(input_ids, segment_ids)\n",
    "    probs = F.softmax(logits, axis=1)\n",
    "    preds = paddle.argmax(probs, axis=1).numpy().tolist()\n",
    "    predictions.extend(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with open('result.txt', 'w') as f:\n",
    "    f.write(idx_to_label[predictions[0]])\n",
    "    for p in predictions[1:]:\n",
    "        f.write('\\n' + idx_to_label[p])\n",
    "    f.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "py35-paddle1.2.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
