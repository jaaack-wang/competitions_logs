{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2022-03-23T20:56:44.654509Z",
     "iopub.status.busy": "2022-03-23T20:56:44.654112Z",
     "iopub.status.idle": "2022-03-23T20:56:44.660285Z",
     "shell.execute_reply": "2022-03-23T20:56:44.659700Z",
     "shell.execute_reply.started": "2022-03-23T20:56:44.654469Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def load_dataset(fpath, test=False, num_row_to_skip=0):\n",
    "    data = open(fpath)\n",
    "    for _ in range(num_row_to_skip):\n",
    "        next(data)\n",
    "\n",
    "    out = []\n",
    "\n",
    "    if test:\n",
    "        for line in data:\n",
    "            out.append(line.strip())\n",
    "        \n",
    "        return out\n",
    "\n",
    "    idx_to_label = {}\n",
    "    for line in data:\n",
    "        line = line.strip().split('\\t')\n",
    "        if len(line) == 3:\n",
    "            idx, label, text = line\n",
    "            idx = int(idx)\n",
    "            idx_to_label[idx] = label\n",
    "            out.append([text, idx])\n",
    "    \n",
    "    return out, idx_to_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2022-03-23T20:56:44.662167Z",
     "iopub.status.busy": "2022-03-23T20:56:44.661718Z",
     "iopub.status.idle": "2022-03-23T20:56:46.402135Z",
     "shell.execute_reply": "2022-03-23T20:56:46.401305Z",
     "shell.execute_reply.started": "2022-03-23T20:56:44.662143Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(752471,\n",
       " [['上证50ETF净申购突增', 0], ['交银施罗德保本基金将发行', 0]],\n",
       " {0: '财经',\n",
       "  1: '彩票',\n",
       "  2: '房产',\n",
       "  3: '股票',\n",
       "  4: '家居',\n",
       "  5: '教育',\n",
       "  6: '科技',\n",
       "  7: '社会',\n",
       "  8: '时尚',\n",
       "  9: '时政',\n",
       "  10: '体育',\n",
       "  11: '星座',\n",
       "  12: '游戏',\n",
       "  13: '娱乐'})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set, idx_to_label = load_dataset('./data/data12701/Train.txt')\n",
    "len(train_set), train_set[:2], idx_to_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2022-03-23T20:56:46.403379Z",
     "iopub.status.busy": "2022-03-23T20:56:46.403181Z",
     "iopub.status.idle": "2022-03-23T20:56:47.211147Z",
     "shell.execute_reply": "2022-03-23T20:56:47.210403Z",
     "shell.execute_reply.started": "2022-03-23T20:56:46.403356Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# split the train_set into train and dev sets\n",
    "from random import shuffle, seed\n",
    "\n",
    "seed(43)\n",
    "shuffle(train_set)\n",
    "\n",
    "train_set, dev_set = train_set[:652471], train_set[652471: ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2022-03-23T20:56:47.213308Z",
     "iopub.status.busy": "2022-03-23T20:56:47.212769Z",
     "iopub.status.idle": "2022-03-23T20:56:47.337044Z",
     "shell.execute_reply": "2022-03-23T20:56:47.336064Z",
     "shell.execute_reply.started": "2022-03-23T20:56:47.213276Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(83599, ['北京君太百货璀璨秋色 满100省353020元', '教育部：小学高年级将开始学习性知识'])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_set = load_dataset('./data/data12701/Test.txt', test=True)\n",
    "len(test_set), test_set[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Transform text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2022-03-23T20:56:47.338706Z",
     "iopub.status.busy": "2022-03-23T20:56:47.338395Z",
     "iopub.status.idle": "2022-03-23T20:56:49.255895Z",
     "shell.execute_reply": "2022-03-23T20:56:49.255203Z",
     "shell.execute_reply.started": "2022-03-23T20:56:47.338680Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-03-24 04:56:49,106] [    INFO] - Downloading https://paddlenlp.bj.bcebos.com/models/transformers/ernie_gram_zh/vocab.txt and saved to /home/aistudio/.paddlenlp/models/ernie-gram-zh\n",
      "[2022-03-24 04:56:49,110] [    INFO] - Downloading vocab.txt from https://paddlenlp.bj.bcebos.com/models/transformers/ernie_gram_zh/vocab.txt\n",
      "100%|██████████| 78/78 [00:00<00:00, 2815.43it/s]\n"
     ]
    }
   ],
   "source": [
    "from paddlenlp.datasets import MapDataset\n",
    "from paddle.io import BatchSampler, DataLoader\n",
    "from paddlenlp.data import Pad, Stack, Tuple\n",
    "from paddlenlp.transformers import ErnieGramForSequenceClassification as SeqClfModel\n",
    "from paddlenlp.transformers import ErnieGramTokenizer as PTMTokenizer\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "MODEL_NAME = \"ernie-gram-zh\"\n",
    "tokenizer = PTMTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "\n",
    "def example_converter(example, tokenizer, max_seq_length=128):\n",
    "    \n",
    "    text, label = example\n",
    "    encoded_inputs = tokenizer(text=text, max_seq_len=max_seq_length)\n",
    "    input_ids = encoded_inputs[\"input_ids\"]\n",
    "    token_type_ids = encoded_inputs[\"token_type_ids\"]\n",
    "    label = np.array([label], dtype=\"int64\")\n",
    "    return input_ids, token_type_ids, label\n",
    "\n",
    "\n",
    "def get_trans_fn(text_encoder, max_seq_length=128):\n",
    "    return lambda ex: example_converter(ex, text_encoder, max_seq_length)\n",
    "\n",
    "\n",
    "def get_batchify_fn(tokenizer=tokenizer):\n",
    "    \n",
    "    batchify_fn = lambda samples, fn=Tuple(\n",
    "        Pad(axis=0, pad_val=tokenizer.pad_token_id), \n",
    "        Pad(axis=0, pad_val=tokenizer.pad_token_type_id),\n",
    "        Stack(dtype=\"int64\")\n",
    "    ): fn(samples)\n",
    "    \n",
    "    return batchify_fn\n",
    "\n",
    "\n",
    "def create_dataloader(dataset, \n",
    "                      trans_fn, \n",
    "                      batchify_fn, \n",
    "                      test=False,\n",
    "                      batch_size=128, \n",
    "                      shuffle=True, \n",
    "                      sampler=BatchSampler):\n",
    "    \n",
    "    if test:\n",
    "        dataset = [[d, 0] for d in dataset]\n",
    "\n",
    "    if not isinstance(dataset, MapDataset):\n",
    "        dataset = MapDataset(dataset)\n",
    "        \n",
    "    dataset.map(trans_fn)\n",
    "    batch_sampler = sampler(dataset, \n",
    "                            shuffle=shuffle, \n",
    "                            batch_size=batch_size)\n",
    "    \n",
    "    dataloder = DataLoader(dataset, \n",
    "                           batch_sampler=batch_sampler, \n",
    "                           collate_fn=batchify_fn)\n",
    "    \n",
    "    return dataloder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2022-03-23T20:56:49.257574Z",
     "iopub.status.busy": "2022-03-23T20:56:49.257208Z",
     "iopub.status.idle": "2022-03-23T20:56:50.322516Z",
     "shell.execute_reply": "2022-03-23T20:56:50.321727Z",
     "shell.execute_reply.started": "2022-03-23T20:56:49.257541Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "max_seq_length = 16; batch_size = 128\n",
    "trans_fn = get_trans_fn(tokenizer, max_seq_length)\n",
    "batchify_fn = get_batchify_fn()\n",
    "train_loader = create_dataloader(train_set, trans_fn, batchify_fn, batch_size=batch_size)\n",
    "dev_loader = create_dataloader(dev_set, trans_fn, batchify_fn, batch_size=batch_size)\n",
    "test_loader = create_dataloader(test_set, trans_fn, batchify_fn, shuffle=False, test=True, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model preparing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2022-03-23T20:56:50.324123Z",
     "iopub.status.busy": "2022-03-23T20:56:50.323795Z",
     "iopub.status.idle": "2022-03-23T20:57:12.965452Z",
     "shell.execute_reply": "2022-03-23T20:57:12.964733Z",
     "shell.execute_reply.started": "2022-03-23T20:56:50.324093Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-03-24 04:56:50,331] [    INFO] - Downloading https://paddlenlp.bj.bcebos.com/models/transformers/ernie_gram_zh/ernie_gram_zh.pdparams and saved to /home/aistudio/.paddlenlp/models/ernie-gram-zh\n",
      "[2022-03-24 04:56:50,333] [    INFO] - Downloading ernie_gram_zh.pdparams from https://paddlenlp.bj.bcebos.com/models/transformers/ernie_gram_zh/ernie_gram_zh.pdparams\n",
      "100%|██████████| 583566/583566 [00:13<00:00, 43730.33it/s]\n",
      "W0324 04:57:03.838611  6098 device_context.cc:447] Please NOTE: device: 0, GPU Compute Capability: 7.0, Driver API Version: 10.1, Runtime API Version: 10.1\n",
      "W0324 04:57:03.843739  6098 device_context.cc:465] device: 0, cuDNN Version: 7.6.\n"
     ]
    }
   ],
   "source": [
    "import paddle \n",
    "from paddlenlp.transformers import LinearDecayWithWarmup\n",
    "\n",
    "\n",
    "model = SeqClfModel.from_pretrained(MODEL_NAME, num_classes=len(idx_to_label))\n",
    "\n",
    "learning_rate = 5e-5; epochs = 3\n",
    "warmup_proportion = 0.1; weight_decay = 0.01\n",
    "num_training_steps = len(train_loader) * epochs\n",
    "lr_scheduler = LinearDecayWithWarmup(learning_rate, num_training_steps, warmup_proportion)\n",
    "optimizer = paddle.optimizer.AdamW(\n",
    "    learning_rate=lr_scheduler,\n",
    "    parameters=model.parameters(),\n",
    "    weight_decay=weight_decay,\n",
    "    apply_decay_param_fun=lambda x: x in [\n",
    "        p.name for n, p in model.named_parameters()\n",
    "        if not any(nd in n for nd in [\"bias\", \"norm\"])\n",
    "    ])\n",
    "\n",
    "criterion = paddle.nn.loss.CrossEntropyLoss()\n",
    "metric = paddle.metric.Accuracy()\n",
    "\n",
    "\n",
    "@paddle.no_grad()\n",
    "def evaluate(model, criterion, metric, data_loader):\n",
    "    \"\"\"\n",
    "    Given a dataset, it evals model and computes the metric.\n",
    "\n",
    "    Args:\n",
    "        model(obj:`paddle.nn.Layer`): A model to classify texts.\n",
    "        data_loader(obj:`paddle.io.DataLoader`): The dataset loader which generates batches.\n",
    "        criterion(obj:`paddle.nn.Layer`): It can compute the loss.\n",
    "        metric(obj:`paddle.metric.Metric`): The evaluation metric.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    metric.reset()\n",
    "    losses = []\n",
    "    for batch in data_loader:\n",
    "        input_ids, token_type_ids, labels = batch\n",
    "        logits = model(input_ids, token_type_ids)\n",
    "        loss = criterion(logits, labels)\n",
    "        losses.append(loss.numpy())\n",
    "        correct = metric.compute(logits, labels)\n",
    "        metric.update(correct)\n",
    "        accu = metric.accumulate()\n",
    "    print(\"eval loss: %.5f, accu: %.5f\" % (np.mean(losses), accu))\n",
    "    model.train()\n",
    "    metric.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2022-03-23T20:57:12.967307Z",
     "iopub.status.busy": "2022-03-23T20:57:12.966547Z",
     "iopub.status.idle": "2022-03-23T21:52:16.927625Z",
     "shell.execute_reply": "2022-03-23T21:52:16.927003Z",
     "shell.execute_reply.started": "2022-03-23T20:57:12.967276Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "global step 100, epoch: 1, batch: 100, loss: 2.37266, acc: 0.14477\n",
      "global step 200, epoch: 1, batch: 200, loss: 1.39645, acc: 0.30281\n",
      "global step 300, epoch: 1, batch: 300, loss: 0.93262, acc: 0.44714\n",
      "global step 400, epoch: 1, batch: 400, loss: 0.41239, acc: 0.53762\n",
      "global step 500, epoch: 1, batch: 500, loss: 0.49551, acc: 0.59947\n",
      "global step 600, epoch: 1, batch: 600, loss: 0.47667, acc: 0.64404\n",
      "global step 700, epoch: 1, batch: 700, loss: 0.45623, acc: 0.67685\n",
      "global step 800, epoch: 1, batch: 800, loss: 0.42427, acc: 0.70267\n",
      "global step 900, epoch: 1, batch: 900, loss: 0.48502, acc: 0.72291\n",
      "global step 1000, epoch: 1, batch: 1000, loss: 0.30667, acc: 0.73973\n",
      "global step 1100, epoch: 1, batch: 1100, loss: 0.33505, acc: 0.75368\n",
      "global step 1200, epoch: 1, batch: 1200, loss: 0.37932, acc: 0.76512\n",
      "global step 1300, epoch: 1, batch: 1300, loss: 0.26609, acc: 0.77524\n",
      "global step 1400, epoch: 1, batch: 1400, loss: 0.24857, acc: 0.78410\n",
      "global step 1500, epoch: 1, batch: 1500, loss: 0.39095, acc: 0.79170\n",
      "global step 1600, epoch: 1, batch: 1600, loss: 0.47716, acc: 0.79815\n",
      "global step 1700, epoch: 1, batch: 1700, loss: 0.45004, acc: 0.80406\n",
      "global step 1800, epoch: 1, batch: 1800, loss: 0.29403, acc: 0.80963\n",
      "global step 1900, epoch: 1, batch: 1900, loss: 0.25013, acc: 0.81472\n",
      "global step 2000, epoch: 1, batch: 2000, loss: 0.29172, acc: 0.81942\n",
      "global step 2100, epoch: 1, batch: 2100, loss: 0.22156, acc: 0.82388\n",
      "global step 2200, epoch: 1, batch: 2200, loss: 0.18651, acc: 0.82790\n",
      "global step 2300, epoch: 1, batch: 2300, loss: 0.26719, acc: 0.83174\n",
      "global step 2400, epoch: 1, batch: 2400, loss: 0.23142, acc: 0.83525\n",
      "global step 2500, epoch: 1, batch: 2500, loss: 0.21915, acc: 0.83837\n",
      "global step 2600, epoch: 1, batch: 2600, loss: 0.29099, acc: 0.84123\n",
      "global step 2700, epoch: 1, batch: 2700, loss: 0.23958, acc: 0.84423\n",
      "global step 2800, epoch: 1, batch: 2800, loss: 0.23989, acc: 0.84663\n",
      "global step 2900, epoch: 1, batch: 2900, loss: 0.21816, acc: 0.84919\n",
      "global step 3000, epoch: 1, batch: 3000, loss: 0.24581, acc: 0.85170\n",
      "global step 3100, epoch: 1, batch: 3100, loss: 0.20661, acc: 0.85402\n",
      "global step 3200, epoch: 1, batch: 3200, loss: 0.24070, acc: 0.85613\n",
      "global step 3300, epoch: 1, batch: 3300, loss: 0.28142, acc: 0.85803\n",
      "global step 3400, epoch: 1, batch: 3400, loss: 0.17850, acc: 0.85990\n",
      "global step 3500, epoch: 1, batch: 3500, loss: 0.20328, acc: 0.86163\n",
      "global step 3600, epoch: 1, batch: 3600, loss: 0.31685, acc: 0.86336\n",
      "global step 3700, epoch: 1, batch: 3700, loss: 0.18563, acc: 0.86511\n",
      "global step 3800, epoch: 1, batch: 3800, loss: 0.27755, acc: 0.86679\n",
      "global step 3900, epoch: 1, batch: 3900, loss: 0.29141, acc: 0.86826\n",
      "global step 4000, epoch: 1, batch: 4000, loss: 0.23354, acc: 0.86964\n",
      "global step 4100, epoch: 1, batch: 4100, loss: 0.27168, acc: 0.87103\n",
      "global step 4200, epoch: 1, batch: 4200, loss: 0.13539, acc: 0.87238\n",
      "global step 4300, epoch: 1, batch: 4300, loss: 0.17119, acc: 0.87371\n",
      "global step 4400, epoch: 1, batch: 4400, loss: 0.25986, acc: 0.87509\n",
      "global step 4500, epoch: 1, batch: 4500, loss: 0.32980, acc: 0.87631\n",
      "global step 4600, epoch: 1, batch: 4600, loss: 0.30204, acc: 0.87742\n",
      "global step 4700, epoch: 1, batch: 4700, loss: 0.14713, acc: 0.87843\n",
      "global step 4800, epoch: 1, batch: 4800, loss: 0.26944, acc: 0.87947\n",
      "global step 4900, epoch: 1, batch: 4900, loss: 0.15432, acc: 0.88053\n",
      "global step 5000, epoch: 1, batch: 5000, loss: 0.21434, acc: 0.88162\n",
      "eval loss: 0.19474, accu: 0.93803\n",
      "global step 5100, epoch: 2, batch: 2, loss: 0.22775, acc: 0.91406\n",
      "global step 5200, epoch: 2, batch: 102, loss: 0.28805, acc: 0.94164\n",
      "global step 5300, epoch: 2, batch: 202, loss: 0.18315, acc: 0.94056\n",
      "global step 5400, epoch: 2, batch: 302, loss: 0.05532, acc: 0.94053\n",
      "global step 5500, epoch: 2, batch: 402, loss: 0.23767, acc: 0.93972\n",
      "global step 5600, epoch: 2, batch: 502, loss: 0.28372, acc: 0.93910\n",
      "global step 5700, epoch: 2, batch: 602, loss: 0.09393, acc: 0.93915\n",
      "global step 5800, epoch: 2, batch: 702, loss: 0.08663, acc: 0.93951\n",
      "global step 5900, epoch: 2, batch: 802, loss: 0.20246, acc: 0.93919\n",
      "global step 6000, epoch: 2, batch: 902, loss: 0.19453, acc: 0.93930\n",
      "global step 6100, epoch: 2, batch: 1002, loss: 0.23676, acc: 0.93950\n",
      "global step 6200, epoch: 2, batch: 1102, loss: 0.14406, acc: 0.93995\n",
      "global step 6300, epoch: 2, batch: 1202, loss: 0.20430, acc: 0.93990\n",
      "global step 6400, epoch: 2, batch: 1302, loss: 0.09158, acc: 0.94013\n",
      "global step 6500, epoch: 2, batch: 1402, loss: 0.20743, acc: 0.93995\n",
      "global step 6600, epoch: 2, batch: 1502, loss: 0.14913, acc: 0.94022\n",
      "global step 6700, epoch: 2, batch: 1602, loss: 0.15250, acc: 0.94033\n",
      "global step 6800, epoch: 2, batch: 1702, loss: 0.14534, acc: 0.94051\n",
      "global step 6900, epoch: 2, batch: 1802, loss: 0.08892, acc: 0.94048\n",
      "global step 7000, epoch: 2, batch: 1902, loss: 0.15725, acc: 0.94047\n",
      "global step 7100, epoch: 2, batch: 2002, loss: 0.23184, acc: 0.94058\n",
      "global step 7200, epoch: 2, batch: 2102, loss: 0.27247, acc: 0.94056\n",
      "global step 7300, epoch: 2, batch: 2202, loss: 0.24019, acc: 0.94058\n",
      "global step 7400, epoch: 2, batch: 2302, loss: 0.12281, acc: 0.94064\n",
      "global step 7500, epoch: 2, batch: 2402, loss: 0.08773, acc: 0.94072\n",
      "global step 7600, epoch: 2, batch: 2502, loss: 0.28059, acc: 0.94084\n",
      "global step 7700, epoch: 2, batch: 2602, loss: 0.14291, acc: 0.94101\n",
      "global step 7800, epoch: 2, batch: 2702, loss: 0.18042, acc: 0.94111\n",
      "global step 7900, epoch: 2, batch: 2802, loss: 0.09976, acc: 0.94131\n",
      "global step 8000, epoch: 2, batch: 2902, loss: 0.10171, acc: 0.94130\n",
      "global step 8100, epoch: 2, batch: 3002, loss: 0.14126, acc: 0.94145\n",
      "global step 8200, epoch: 2, batch: 3102, loss: 0.15899, acc: 0.94157\n",
      "global step 8300, epoch: 2, batch: 3202, loss: 0.23944, acc: 0.94162\n",
      "global step 8400, epoch: 2, batch: 3302, loss: 0.17187, acc: 0.94170\n",
      "global step 8500, epoch: 2, batch: 3402, loss: 0.22380, acc: 0.94177\n",
      "global step 8600, epoch: 2, batch: 3502, loss: 0.29347, acc: 0.94185\n",
      "global step 8700, epoch: 2, batch: 3602, loss: 0.23506, acc: 0.94184\n",
      "global step 8800, epoch: 2, batch: 3702, loss: 0.11876, acc: 0.94194\n",
      "global step 8900, epoch: 2, batch: 3802, loss: 0.14836, acc: 0.94206\n",
      "global step 9000, epoch: 2, batch: 3902, loss: 0.33989, acc: 0.94215\n",
      "global step 9100, epoch: 2, batch: 4002, loss: 0.09944, acc: 0.94224\n",
      "global step 9200, epoch: 2, batch: 4102, loss: 0.14609, acc: 0.94229\n",
      "global step 9300, epoch: 2, batch: 4202, loss: 0.10795, acc: 0.94235\n",
      "global step 9400, epoch: 2, batch: 4302, loss: 0.18043, acc: 0.94245\n",
      "global step 9500, epoch: 2, batch: 4402, loss: 0.22526, acc: 0.94245\n",
      "global step 9600, epoch: 2, batch: 4502, loss: 0.16380, acc: 0.94251\n",
      "global step 9700, epoch: 2, batch: 4602, loss: 0.08326, acc: 0.94265\n",
      "global step 9800, epoch: 2, batch: 4702, loss: 0.14626, acc: 0.94276\n",
      "global step 9900, epoch: 2, batch: 4802, loss: 0.08963, acc: 0.94289\n",
      "global step 10000, epoch: 2, batch: 4902, loss: 0.09781, acc: 0.94296\n",
      "global step 10100, epoch: 2, batch: 5002, loss: 0.14527, acc: 0.94302\n",
      "eval loss: 0.16822, accu: 0.94610\n",
      "global step 10200, epoch: 3, batch: 4, loss: 0.06876, acc: 0.97656\n",
      "global step 10300, epoch: 3, batch: 104, loss: 0.12080, acc: 0.96199\n",
      "global step 10400, epoch: 3, batch: 204, loss: 0.25334, acc: 0.96048\n",
      "global step 10500, epoch: 3, batch: 304, loss: 0.10970, acc: 0.95940\n",
      "global step 10600, epoch: 3, batch: 404, loss: 0.17774, acc: 0.95916\n",
      "global step 10700, epoch: 3, batch: 504, loss: 0.18546, acc: 0.95895\n",
      "global step 10800, epoch: 3, batch: 604, loss: 0.10868, acc: 0.95849\n",
      "global step 10900, epoch: 3, batch: 704, loss: 0.12574, acc: 0.95873\n",
      "global step 11000, epoch: 3, batch: 804, loss: 0.12061, acc: 0.95869\n",
      "global step 11100, epoch: 3, batch: 904, loss: 0.24570, acc: 0.95830\n",
      "global step 11200, epoch: 3, batch: 1004, loss: 0.08724, acc: 0.95835\n",
      "global step 11300, epoch: 3, batch: 1104, loss: 0.08318, acc: 0.95839\n",
      "global step 11400, epoch: 3, batch: 1204, loss: 0.08982, acc: 0.95865\n",
      "global step 11500, epoch: 3, batch: 1304, loss: 0.15686, acc: 0.95863\n",
      "global step 11600, epoch: 3, batch: 1404, loss: 0.16040, acc: 0.95853\n",
      "global step 11700, epoch: 3, batch: 1504, loss: 0.06482, acc: 0.95846\n",
      "global step 11800, epoch: 3, batch: 1604, loss: 0.23163, acc: 0.95837\n",
      "global step 11900, epoch: 3, batch: 1704, loss: 0.11779, acc: 0.95824\n",
      "global step 12000, epoch: 3, batch: 1804, loss: 0.09871, acc: 0.95835\n",
      "global step 12100, epoch: 3, batch: 1904, loss: 0.07689, acc: 0.95829\n",
      "global step 12200, epoch: 3, batch: 2004, loss: 0.14041, acc: 0.95831\n",
      "global step 12300, epoch: 3, batch: 2104, loss: 0.17361, acc: 0.95824\n",
      "global step 12400, epoch: 3, batch: 2204, loss: 0.18769, acc: 0.95827\n",
      "global step 12500, epoch: 3, batch: 2304, loss: 0.10167, acc: 0.95833\n",
      "global step 12600, epoch: 3, batch: 2404, loss: 0.09823, acc: 0.95839\n",
      "global step 12700, epoch: 3, batch: 2504, loss: 0.10246, acc: 0.95851\n",
      "global step 12800, epoch: 3, batch: 2604, loss: 0.18028, acc: 0.95854\n",
      "global step 12900, epoch: 3, batch: 2704, loss: 0.16829, acc: 0.95844\n",
      "global step 13000, epoch: 3, batch: 2804, loss: 0.10761, acc: 0.95854\n",
      "global step 13100, epoch: 3, batch: 2904, loss: 0.14139, acc: 0.95867\n",
      "global step 13200, epoch: 3, batch: 3004, loss: 0.14938, acc: 0.95875\n",
      "global step 13300, epoch: 3, batch: 3104, loss: 0.11589, acc: 0.95873\n",
      "global step 13400, epoch: 3, batch: 3204, loss: 0.14925, acc: 0.95880\n",
      "global step 13500, epoch: 3, batch: 3304, loss: 0.07695, acc: 0.95879\n",
      "global step 13600, epoch: 3, batch: 3404, loss: 0.14790, acc: 0.95875\n",
      "global step 13700, epoch: 3, batch: 3504, loss: 0.22673, acc: 0.95879\n",
      "global step 13800, epoch: 3, batch: 3604, loss: 0.11061, acc: 0.95880\n",
      "global step 13900, epoch: 3, batch: 3704, loss: 0.06772, acc: 0.95885\n",
      "global step 14000, epoch: 3, batch: 3804, loss: 0.04761, acc: 0.95887\n",
      "global step 14100, epoch: 3, batch: 3904, loss: 0.15604, acc: 0.95895\n",
      "global step 14200, epoch: 3, batch: 4004, loss: 0.08362, acc: 0.95904\n",
      "global step 14300, epoch: 3, batch: 4104, loss: 0.12045, acc: 0.95911\n",
      "global step 14400, epoch: 3, batch: 4204, loss: 0.11265, acc: 0.95908\n",
      "global step 14500, epoch: 3, batch: 4304, loss: 0.17832, acc: 0.95912\n",
      "global step 14600, epoch: 3, batch: 4404, loss: 0.24083, acc: 0.95912\n",
      "global step 14700, epoch: 3, batch: 4504, loss: 0.10663, acc: 0.95922\n",
      "global step 14800, epoch: 3, batch: 4604, loss: 0.03238, acc: 0.95926\n",
      "global step 14900, epoch: 3, batch: 4704, loss: 0.12475, acc: 0.95934\n",
      "global step 15000, epoch: 3, batch: 4804, loss: 0.02582, acc: 0.95943\n",
      "global step 15100, epoch: 3, batch: 4904, loss: 0.11638, acc: 0.95949\n",
      "global step 15200, epoch: 3, batch: 5004, loss: 0.04511, acc: 0.95952\n",
      "eval loss: 0.15709, accu: 0.95107\n"
     ]
    }
   ],
   "source": [
    "import paddle.nn.functional as F\n",
    "\n",
    "\n",
    "global_step = 0\n",
    "paddle.set_device(\"gpu\")\n",
    "for epoch in range(1, epochs + 1):\n",
    "    for step, batch in enumerate(train_loader, start=1):\n",
    "        input_ids, segment_ids, labels = batch\n",
    "        logits = model(input_ids, segment_ids)\n",
    "        loss = criterion(logits, labels)\n",
    "        probs = F.softmax(logits, axis=1)\n",
    "        correct = metric.compute(probs, labels)\n",
    "        metric.update(correct)\n",
    "        acc = metric.accumulate()\n",
    "\n",
    "        global_step += 1\n",
    "        if global_step % 100 == 0 :\n",
    "            print(\"global step %d, epoch: %d, batch: %d, loss: %.5f, acc: %.5f\" % (global_step, epoch, step, loss, acc))\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        optimizer.clear_grad()\n",
    "    evaluate(model, criterion, metric, dev_loader)\n",
    "\n",
    "\n",
    "# model.save_pretrained('/home/aistudio/checkpoint')\n",
    "# tokenizer.save_pretrained('/home/aistudio/checkpoint')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2022-03-23T21:52:16.930109Z",
     "iopub.status.busy": "2022-03-23T21:52:16.929771Z",
     "iopub.status.idle": "2022-03-23T21:52:53.785010Z",
     "shell.execute_reply": "2022-03-23T21:52:53.784071Z",
     "shell.execute_reply.started": "2022-03-23T21:52:16.930067Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "predictions = []\n",
    "\n",
    "for batch in test_loader:\n",
    "    input_ids, segment_ids, _ = batch\n",
    "    logits = model(input_ids, segment_ids)\n",
    "    probs = F.softmax(logits, axis=1)\n",
    "    preds = paddle.argmax(probs, axis=1).numpy().tolist()\n",
    "    predictions.extend(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2022-03-23T21:52:53.787090Z",
     "iopub.status.busy": "2022-03-23T21:52:53.786572Z",
     "iopub.status.idle": "2022-03-23T21:52:53.814200Z",
     "shell.execute_reply": "2022-03-23T21:52:53.813674Z",
     "shell.execute_reply.started": "2022-03-23T21:52:53.787056Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with open('result.txt', 'w') as f:\n",
    "    f.write(idx_to_label[predictions[0]])\n",
    "    for p in predictions[1:]:\n",
    "        f.write('\\n' + idx_to_label[p])\n",
    "    f.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "py35-paddle1.2.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
