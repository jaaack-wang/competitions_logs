{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-28T20:19:19.753067Z",
     "iopub.status.busy": "2022-03-28T20:19:19.752785Z",
     "iopub.status.idle": "2022-03-28T20:19:19.755358Z",
     "shell.execute_reply": "2022-03-28T20:19:19.754890Z",
     "shell.execute_reply.started": "2022-03-28T20:19:19.753037Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# !unzip /home/aistudio/data/data78992/lcqmc.zip -d /home/aistudio/data/\n",
    "# !unzip /home/aistudio/data/data78992/paws-x-zh.zip -d /home/aistudio/data/\n",
    "# !unzip /home/aistudio/data/data78992/bq_corpus.zip -d /home/aistudio/data/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-28T20:19:19.756268Z",
     "iopub.status.busy": "2022-03-28T20:19:19.756016Z",
     "iopub.status.idle": "2022-03-28T20:19:19.762234Z",
     "shell.execute_reply": "2022-03-28T20:19:19.761795Z",
     "shell.execute_reply.started": "2022-03-28T20:19:19.756247Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def load_dataset(fpath, num_row_skip=0):\n",
    "\n",
    "    def read(fp):\n",
    "        data = open(fp)\n",
    "\n",
    "        for _ in range(num_row_skip):\n",
    "            next(data)\n",
    "\n",
    "        if \"test\" in fp:\n",
    "            for line in data:\n",
    "                line = line.strip().split('\\t')\n",
    "                yield line[0], line[1]\n",
    "        else:\n",
    "            for line in data:\n",
    "                line = line.strip().split('\\t')\n",
    "                if len(line) == 3:\n",
    "                    yield line[0], line[1], int(line[2])\n",
    "\n",
    "    if isinstance(fpath, str):\n",
    "        return list(read(fpath))\n",
    "    elif isinstance(fpath, (list, tuple)):\n",
    "        return [list(read(fp)) for fp in fpath]\n",
    "    else:\n",
    "        raise TypeError(\"Input fpath must be a str or a list/tuple of str\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-28T20:19:19.797216Z",
     "iopub.status.busy": "2022-03-28T20:19:19.797026Z",
     "iopub.status.idle": "2022-03-28T20:19:19.977638Z",
     "shell.execute_reply": "2022-03-28T20:19:19.977152Z",
     "shell.execute_reply.started": "2022-03-28T20:19:19.797196Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_set, dev_set, test_set = load_dataset(['./data/bq_corpus/train.tsv', './data/bq_corpus/dev.tsv', './data/bq_corpus/test.tsv'])\n",
    "# len(train_set), len(dev_set), len(test_set)\n",
    "train_set = train_set + dev_set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Transform text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-28T20:19:19.979123Z",
     "iopub.status.busy": "2022-03-28T20:19:19.978741Z",
     "iopub.status.idle": "2022-03-28T20:19:21.597474Z",
     "shell.execute_reply": "2022-03-28T20:19:21.596943Z",
     "shell.execute_reply.started": "2022-03-28T20:19:19.979098Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-03-29 04:19:21,581] [    INFO] - Already cached /home/aistudio/.paddlenlp/models/roberta-wwm-ext-large/vocab.txt\n"
     ]
    }
   ],
   "source": [
    "from paddlenlp.datasets import MapDataset\n",
    "from paddle.io import BatchSampler, DataLoader\n",
    "from paddlenlp.data import Pad, Stack, Tuple\n",
    "from paddlenlp.transformers import RobertaModel as SeqClfModel\n",
    "from paddlenlp.transformers import RobertaTokenizer as PTMTokenizer\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "MODEL_NAME = \"roberta-wwm-ext-large\"\n",
    "tokenizer = PTMTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "\n",
    "def example_converter(example, max_seq_length, tokenizer):\n",
    "    text_a, text_b, label = example\n",
    "    encoded = tokenizer(text=text_a, text_pair=text_b, max_seq_len=max_seq_length)\n",
    "    input_ids = encoded[\"input_ids\"]\n",
    "    token_type_ids = encoded[\"token_type_ids\"]\n",
    "    label = np.array([label], dtype=\"int64\")\n",
    "    return input_ids, token_type_ids, label\n",
    "\n",
    "\n",
    "def get_trans_fn(max_seq_length=128, tokenizer=tokenizer):\n",
    "    return lambda ex: example_converter(ex, max_seq_length, tokenizer)\n",
    "\n",
    "\n",
    "batchify_fn = lambda samples, fn=Tuple(\n",
    "    Pad(axis=0, pad_val=tokenizer.pad_token_id), \n",
    "    Pad(axis=0, pad_val=tokenizer.pad_token_type_id),\n",
    "    Stack(dtype=\"int64\")\n",
    "    ): fn(samples)\n",
    "\n",
    "\n",
    "def create_dataloader(dataset, \n",
    "                      trans_fn, \n",
    "                      batchify_fn, \n",
    "                      test=False,\n",
    "                      batch_size=128, \n",
    "                      shuffle=True, \n",
    "                      sampler=BatchSampler):\n",
    "    \n",
    "    if test:\n",
    "        dataset = [d + (0,) for d in dataset]\n",
    "\n",
    "    if not isinstance(dataset, MapDataset):\n",
    "        dataset = MapDataset(dataset)\n",
    "        \n",
    "    dataset.map(trans_fn)\n",
    "    batch_sampler = sampler(dataset, \n",
    "                            shuffle=shuffle, \n",
    "                            batch_size=batch_size)\n",
    "    \n",
    "    dataloder = DataLoader(dataset, \n",
    "                           batch_sampler=batch_sampler, \n",
    "                           collate_fn=batchify_fn)\n",
    "    \n",
    "    return dataloder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-28T20:19:21.598624Z",
     "iopub.status.busy": "2022-03-28T20:19:21.598358Z",
     "iopub.status.idle": "2022-03-28T20:19:21.605778Z",
     "shell.execute_reply": "2022-03-28T20:19:21.605314Z",
     "shell.execute_reply.started": "2022-03-28T20:19:21.598598Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "max_seq_length = 128; batch_size = 32\n",
    "trans_fn = get_trans_fn(max_seq_length)\n",
    "train_loader = create_dataloader(train_set, trans_fn, batchify_fn, batch_size=batch_size)\n",
    "# dev_loader = create_dataloader(dev_set, trans_fn, batchify_fn, batch_size=batch_size)\n",
    "test_loader = create_dataloader(test_set, trans_fn, batchify_fn, shuffle=False, test=True, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-28T20:19:21.606732Z",
     "iopub.status.busy": "2022-03-28T20:19:21.606520Z",
     "iopub.status.idle": "2022-03-28T20:19:21.612513Z",
     "shell.execute_reply": "2022-03-28T20:19:21.612065Z",
     "shell.execute_reply.started": "2022-03-28T20:19:21.606710Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from paddle import nn\n",
    "import paddle\n",
    "\n",
    "\n",
    "class PTM(nn.Layer):\n",
    "\n",
    "    def __init__(self, pretrained_model, dropout=0.1, num_class=2):\n",
    "        super().__init__()\n",
    "\n",
    "        self.ptm = pretrained_model\n",
    "        ptm_out_dim = self.ptm.config[\"hidden_size\"]\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc1 = nn.Linear(ptm_out_dim, ptm_out_dim // 2)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(ptm_out_dim // 2, num_class)\n",
    "\n",
    "    def encoder(self, input_ids, token_type_ids):\n",
    "        _, embd = self.ptm(input_ids, token_type_ids)\n",
    "        embd = self.dropout(embd)\n",
    "        return embd\n",
    "\n",
    "    def forward(self, input_ids, token_type_ids):\n",
    "        embd = self.encoder(input_ids, token_type_ids)\n",
    "        hidden = self.relu(self.fc1(embd))\n",
    "        logits = self.fc2(hidden)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-28T20:19:21.614033Z",
     "iopub.status.busy": "2022-03-28T20:19:21.613814Z",
     "iopub.status.idle": "2022-03-28T20:19:21.619120Z",
     "shell.execute_reply": "2022-03-28T20:19:21.618662Z",
     "shell.execute_reply.started": "2022-03-28T20:19:21.614012Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from paddlenlp.transformers import LinearDecayWithWarmup\n",
    "\n",
    "epoch = 4\n",
    "weight_decay = 0.0\n",
    "warmup_proportion = 0.0\n",
    "lr_scheduler = LinearDecayWithWarmup(2e-5, len(train_loader) * epoch,\n",
    "                                         warmup_proportion)\n",
    "\n",
    "def get_model(model):\n",
    "    decay_params = [\n",
    "        p.name for n, p in model.named_parameters()\n",
    "        if not any(nd in n for nd in [\"bias\", \"norm\"])\n",
    "    ]\n",
    "    optimizer = paddle.optimizer.AdamW(\n",
    "    parameters=model.parameters(), \n",
    "    learning_rate=lr_scheduler, \n",
    "    weight_decay=weight_decay, \n",
    "    apply_decay_param_fun=lambda x: x in decay_params)\n",
    "\n",
    "    criterion = paddle.nn.CrossEntropyLoss()\n",
    "\n",
    "    model = paddle.Model(model)\n",
    "    metric = paddle.metric.Accuracy()\n",
    "    model.prepare(optimizer, criterion, metric)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-28T20:19:21.620027Z",
     "iopub.status.busy": "2022-03-28T20:19:21.619750Z",
     "iopub.status.idle": "2022-03-28T20:19:28.643756Z",
     "shell.execute_reply": "2022-03-28T20:19:28.643129Z",
     "shell.execute_reply.started": "2022-03-28T20:19:21.619997Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-03-29 04:19:21,621] [    INFO] - Already cached /home/aistudio/.paddlenlp/models/roberta-wwm-ext-large/roberta_chn_large.pdparams\n",
      "W0329 04:19:21.623505  1556 device_context.cc:447] Please NOTE: device: 0, GPU Compute Capability: 7.0, Driver API Version: 10.1, Runtime API Version: 10.1\n",
      "W0329 04:19:21.627106  1556 device_context.cc:465] device: 0, cuDNN Version: 7.6.\n"
     ]
    }
   ],
   "source": [
    "ptm = SeqClfModel.from_pretrained(MODEL_NAME)\n",
    "model = PTM(ptm)\n",
    "model = get_model(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-28T20:19:28.645119Z",
     "iopub.status.busy": "2022-03-28T20:19:28.644771Z",
     "iopub.status.idle": "2022-03-28T21:37:17.599557Z",
     "shell.execute_reply": "2022-03-28T21:37:17.598975Z",
     "shell.execute_reply.started": "2022-03-28T20:19:28.645093Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The loss value printed in the log is the current step, and the metric is the average value of previous steps.\n",
      "Epoch 1/4\n",
      "step  100/3438 - loss: 0.5379 - acc: 0.6462 - 334ms/step\n",
      "step  200/3438 - loss: 0.4260 - acc: 0.7072 - 332ms/step\n",
      "step  300/3438 - loss: 0.3677 - acc: 0.7335 - 335ms/step\n",
      "step  400/3438 - loss: 0.6715 - acc: 0.7488 - 337ms/step\n",
      "step  500/3438 - loss: 0.3896 - acc: 0.7637 - 340ms/step\n",
      "step  600/3438 - loss: 0.3720 - acc: 0.7730 - 342ms/step\n",
      "step  700/3438 - loss: 0.4872 - acc: 0.7813 - 342ms/step\n",
      "step  800/3438 - loss: 0.3558 - acc: 0.7872 - 343ms/step\n",
      "step  900/3438 - loss: 0.2119 - acc: 0.7915 - 342ms/step\n",
      "step 1000/3438 - loss: 0.2769 - acc: 0.7967 - 341ms/step\n",
      "step 1100/3438 - loss: 0.3397 - acc: 0.8007 - 340ms/step\n",
      "step 1200/3438 - loss: 0.3669 - acc: 0.8048 - 340ms/step\n",
      "step 1300/3438 - loss: 0.4902 - acc: 0.8086 - 340ms/step\n",
      "step 1400/3438 - loss: 0.4080 - acc: 0.8121 - 341ms/step\n",
      "step 1500/3438 - loss: 0.4065 - acc: 0.8156 - 341ms/step\n",
      "step 1600/3438 - loss: 0.2256 - acc: 0.8191 - 341ms/step\n",
      "step 1700/3438 - loss: 0.2179 - acc: 0.8224 - 341ms/step\n",
      "step 1800/3438 - loss: 0.4236 - acc: 0.8254 - 341ms/step\n",
      "step 1900/3438 - loss: 0.2028 - acc: 0.8275 - 341ms/step\n",
      "step 2000/3438 - loss: 0.3723 - acc: 0.8298 - 341ms/step\n",
      "step 2100/3438 - loss: 0.2147 - acc: 0.8323 - 341ms/step\n",
      "step 2200/3438 - loss: 0.2451 - acc: 0.8343 - 341ms/step\n",
      "step 2300/3438 - loss: 0.3248 - acc: 0.8365 - 341ms/step\n",
      "step 2400/3438 - loss: 0.2419 - acc: 0.8383 - 341ms/step\n",
      "step 2500/3438 - loss: 0.2058 - acc: 0.8403 - 341ms/step\n",
      "step 2600/3438 - loss: 0.0790 - acc: 0.8424 - 341ms/step\n",
      "step 2700/3438 - loss: 0.2710 - acc: 0.8441 - 341ms/step\n",
      "step 2800/3438 - loss: 0.3481 - acc: 0.8461 - 341ms/step\n",
      "step 2900/3438 - loss: 0.4306 - acc: 0.8475 - 340ms/step\n",
      "step 3000/3438 - loss: 0.3176 - acc: 0.8492 - 340ms/step\n",
      "step 3100/3438 - loss: 0.1922 - acc: 0.8509 - 340ms/step\n",
      "step 3200/3438 - loss: 0.2681 - acc: 0.8522 - 341ms/step\n",
      "step 3300/3438 - loss: 0.3484 - acc: 0.8537 - 341ms/step\n",
      "step 3400/3438 - loss: 0.1314 - acc: 0.8549 - 340ms/step\n",
      "step 3438/3438 - loss: 0.3225 - acc: 0.8553 - 340ms/step\n",
      "Epoch 2/4\n",
      "step  100/3438 - loss: 0.1004 - acc: 0.9287 - 338ms/step\n",
      "step  200/3438 - loss: 0.1015 - acc: 0.9295 - 342ms/step\n",
      "step  300/3438 - loss: 0.0815 - acc: 0.9291 - 338ms/step\n",
      "step  400/3438 - loss: 0.1559 - acc: 0.9308 - 337ms/step\n",
      "step  500/3438 - loss: 0.0847 - acc: 0.9311 - 336ms/step\n",
      "step  600/3438 - loss: 0.1583 - acc: 0.9314 - 335ms/step\n",
      "step  700/3438 - loss: 0.1611 - acc: 0.9311 - 336ms/step\n",
      "step  800/3438 - loss: 0.0369 - acc: 0.9321 - 337ms/step\n",
      "step  900/3438 - loss: 0.1514 - acc: 0.9316 - 338ms/step\n",
      "step 1000/3438 - loss: 0.3877 - acc: 0.9320 - 337ms/step\n",
      "step 1100/3438 - loss: 0.2019 - acc: 0.9328 - 337ms/step\n",
      "step 1200/3438 - loss: 0.4915 - acc: 0.9333 - 337ms/step\n",
      "step 1300/3438 - loss: 0.2430 - acc: 0.9335 - 338ms/step\n",
      "step 1400/3438 - loss: 0.2161 - acc: 0.9335 - 339ms/step\n",
      "step 1500/3438 - loss: 0.2031 - acc: 0.9339 - 338ms/step\n",
      "step 1600/3438 - loss: 0.1077 - acc: 0.9346 - 338ms/step\n",
      "step 1700/3438 - loss: 0.0448 - acc: 0.9350 - 338ms/step\n",
      "step 1800/3438 - loss: 0.5496 - acc: 0.9354 - 338ms/step\n",
      "step 1900/3438 - loss: 0.0816 - acc: 0.9355 - 339ms/step\n",
      "step 2000/3438 - loss: 0.0914 - acc: 0.9357 - 339ms/step\n",
      "step 2100/3438 - loss: 0.1856 - acc: 0.9356 - 339ms/step\n",
      "step 2200/3438 - loss: 0.0876 - acc: 0.9359 - 339ms/step\n",
      "step 2300/3438 - loss: 0.0843 - acc: 0.9362 - 339ms/step\n",
      "step 2400/3438 - loss: 0.1274 - acc: 0.9365 - 339ms/step\n",
      "step 2500/3438 - loss: 0.0667 - acc: 0.9365 - 339ms/step\n",
      "step 2600/3438 - loss: 0.1668 - acc: 0.9367 - 339ms/step\n",
      "step 2700/3438 - loss: 0.2686 - acc: 0.9370 - 339ms/step\n",
      "step 2800/3438 - loss: 0.1516 - acc: 0.9375 - 339ms/step\n",
      "step 2900/3438 - loss: 0.4341 - acc: 0.9379 - 339ms/step\n",
      "step 3000/3438 - loss: 0.0616 - acc: 0.9383 - 339ms/step\n",
      "step 3100/3438 - loss: 0.0451 - acc: 0.9385 - 339ms/step\n",
      "step 3200/3438 - loss: 0.0452 - acc: 0.9388 - 339ms/step\n",
      "step 3300/3438 - loss: 0.1958 - acc: 0.9393 - 340ms/step\n",
      "step 3400/3438 - loss: 0.4020 - acc: 0.9396 - 340ms/step\n",
      "step 3438/3438 - loss: 0.2171 - acc: 0.9397 - 340ms/step\n",
      "Epoch 3/4\n",
      "step  100/3438 - loss: 0.2624 - acc: 0.9678 - 337ms/step\n",
      "step  200/3438 - loss: 0.0671 - acc: 0.9705 - 335ms/step\n",
      "step  300/3438 - loss: 0.0273 - acc: 0.9704 - 338ms/step\n",
      "step  400/3438 - loss: 0.0429 - acc: 0.9717 - 343ms/step\n",
      "step  500/3438 - loss: 0.0369 - acc: 0.9724 - 341ms/step\n",
      "step  600/3438 - loss: 0.0668 - acc: 0.9726 - 343ms/step\n",
      "step  700/3438 - loss: 0.1049 - acc: 0.9725 - 342ms/step\n",
      "step  800/3438 - loss: 0.1174 - acc: 0.9726 - 341ms/step\n",
      "step  900/3438 - loss: 0.0864 - acc: 0.9727 - 341ms/step\n",
      "step 1000/3438 - loss: 0.0750 - acc: 0.9729 - 341ms/step\n",
      "step 1100/3438 - loss: 0.0254 - acc: 0.9730 - 342ms/step\n",
      "step 1200/3438 - loss: 0.0796 - acc: 0.9728 - 342ms/step\n",
      "step 1300/3438 - loss: 0.0807 - acc: 0.9729 - 342ms/step\n",
      "step 1400/3438 - loss: 0.0050 - acc: 0.9730 - 342ms/step\n",
      "step 1500/3438 - loss: 0.0357 - acc: 0.9728 - 341ms/step\n",
      "step 1600/3438 - loss: 0.0173 - acc: 0.9732 - 341ms/step\n",
      "step 1700/3438 - loss: 0.0182 - acc: 0.9732 - 341ms/step\n",
      "step 1800/3438 - loss: 0.0043 - acc: 0.9734 - 341ms/step\n",
      "step 1900/3438 - loss: 0.0554 - acc: 0.9733 - 341ms/step\n",
      "step 2000/3438 - loss: 0.0133 - acc: 0.9733 - 341ms/step\n",
      "step 2100/3438 - loss: 0.0505 - acc: 0.9735 - 340ms/step\n",
      "step 2200/3438 - loss: 0.0045 - acc: 0.9737 - 340ms/step\n",
      "step 2300/3438 - loss: 0.1195 - acc: 0.9738 - 339ms/step\n",
      "step 2400/3438 - loss: 0.0050 - acc: 0.9739 - 340ms/step\n",
      "step 2500/3438 - loss: 0.0732 - acc: 0.9742 - 339ms/step\n",
      "step 2600/3438 - loss: 0.0358 - acc: 0.9742 - 339ms/step\n",
      "step 2700/3438 - loss: 0.0623 - acc: 0.9741 - 339ms/step\n",
      "step 2800/3438 - loss: 0.0083 - acc: 0.9741 - 339ms/step\n",
      "step 2900/3438 - loss: 0.0684 - acc: 0.9744 - 339ms/step\n",
      "step 3000/3438 - loss: 0.0058 - acc: 0.9743 - 339ms/step\n",
      "step 3100/3438 - loss: 0.1107 - acc: 0.9745 - 339ms/step\n",
      "step 3200/3438 - loss: 0.1527 - acc: 0.9745 - 340ms/step\n",
      "step 3300/3438 - loss: 0.0224 - acc: 0.9745 - 339ms/step\n",
      "step 3400/3438 - loss: 0.0362 - acc: 0.9746 - 339ms/step\n",
      "step 3438/3438 - loss: 0.0029 - acc: 0.9746 - 339ms/step\n",
      "Epoch 4/4\n",
      "step  100/3438 - loss: 0.0225 - acc: 0.9866 - 332ms/step\n",
      "step  200/3438 - loss: 0.0184 - acc: 0.9903 - 333ms/step\n",
      "step  300/3438 - loss: 0.0025 - acc: 0.9898 - 330ms/step\n",
      "step  400/3438 - loss: 0.0899 - acc: 0.9902 - 330ms/step\n",
      "step  500/3438 - loss: 0.0126 - acc: 0.9901 - 331ms/step\n",
      "step  600/3438 - loss: 0.0084 - acc: 0.9903 - 331ms/step\n",
      "step  700/3438 - loss: 0.0021 - acc: 0.9907 - 334ms/step\n",
      "step  800/3438 - loss: 0.0018 - acc: 0.9907 - 333ms/step\n",
      "step  900/3438 - loss: 0.0022 - acc: 0.9910 - 333ms/step\n",
      "step 1000/3438 - loss: 0.1345 - acc: 0.9908 - 334ms/step\n",
      "step 1100/3438 - loss: 0.0029 - acc: 0.9903 - 335ms/step\n",
      "step 1200/3438 - loss: 0.0093 - acc: 0.9903 - 336ms/step\n",
      "step 1300/3438 - loss: 0.0065 - acc: 0.9904 - 337ms/step\n",
      "step 1400/3438 - loss: 7.7414e-04 - acc: 0.9904 - 337ms/step\n",
      "step 1500/3438 - loss: 0.0155 - acc: 0.9906 - 338ms/step\n",
      "step 1600/3438 - loss: 0.0015 - acc: 0.9906 - 338ms/step\n",
      "step 1700/3438 - loss: 0.0165 - acc: 0.9905 - 338ms/step\n",
      "step 1800/3438 - loss: 0.0114 - acc: 0.9905 - 338ms/step\n",
      "step 1900/3438 - loss: 0.0020 - acc: 0.9905 - 339ms/step\n",
      "step 2000/3438 - loss: 0.0122 - acc: 0.9905 - 339ms/step\n",
      "step 2100/3438 - loss: 0.0101 - acc: 0.9906 - 340ms/step\n",
      "step 2200/3438 - loss: 0.0028 - acc: 0.9906 - 339ms/step\n",
      "step 2300/3438 - loss: 0.0046 - acc: 0.9906 - 340ms/step\n",
      "step 2400/3438 - loss: 0.0024 - acc: 0.9907 - 340ms/step\n",
      "step 2500/3438 - loss: 0.0023 - acc: 0.9907 - 339ms/step\n",
      "step 2600/3438 - loss: 0.1507 - acc: 0.9908 - 339ms/step\n",
      "step 2700/3438 - loss: 0.0432 - acc: 0.9908 - 340ms/step\n",
      "step 2800/3438 - loss: 0.0021 - acc: 0.9909 - 339ms/step\n",
      "step 2900/3438 - loss: 0.0017 - acc: 0.9909 - 339ms/step\n",
      "step 3000/3438 - loss: 0.0015 - acc: 0.9910 - 339ms/step\n",
      "step 3100/3438 - loss: 0.0056 - acc: 0.9910 - 339ms/step\n",
      "step 3200/3438 - loss: 6.8734e-04 - acc: 0.9911 - 339ms/step\n",
      "step 3300/3438 - loss: 0.0535 - acc: 0.9911 - 339ms/step\n",
      "step 3400/3438 - loss: 0.0086 - acc: 0.9911 - 339ms/step\n",
      "step 3438/3438 - loss: 0.0012 - acc: 0.9911 - 339ms/step\n"
     ]
    }
   ],
   "source": [
    "model.fit(train_loader, epochs=epoch, verbose=2, log_freq=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-28T21:37:17.601788Z",
     "iopub.status.busy": "2022-03-28T21:37:17.600862Z",
     "iopub.status.idle": "2022-03-28T21:37:54.029566Z",
     "shell.execute_reply": "2022-03-28T21:37:54.029012Z",
     "shell.execute_reply.started": "2022-03-28T21:37:17.601730Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predict begin...\n",
      "step 313/313 [==============================] - ETA: 33s - 108ms/ste - ETA: 31s - 101ms/ste - ETA: 33s - 109ms/ste - ETA: 34s - 114ms/ste - ETA: 37s - 125ms/ste - ETA: 37s - 124ms/ste - ETA: 36s - 123ms/ste - ETA: 36s - 122ms/ste - ETA: 35s - 120ms/ste - ETA: 34s - 118ms/ste - ETA: 34s - 119ms/ste - ETA: 33s - 118ms/ste - ETA: 35s - 122ms/ste - ETA: 34s - 121ms/ste - ETA: 34s - 123ms/ste - ETA: 34s - 122ms/ste - ETA: 33s - 120ms/ste - ETA: 33s - 122ms/ste - ETA: 33s - 122ms/ste - ETA: 33s - 122ms/ste - ETA: 33s - 124ms/ste - ETA: 33s - 123ms/ste - ETA: 32s - 122ms/ste - ETA: 32s - 122ms/ste - ETA: 31s - 121ms/ste - ETA: 31s - 120ms/ste - ETA: 30s - 120ms/ste - ETA: 30s - 118ms/ste - ETA: 30s - 118ms/ste - ETA: 29s - 117ms/ste - ETA: 29s - 117ms/ste - ETA: 28s - 116ms/ste - ETA: 28s - 116ms/ste - ETA: 28s - 115ms/ste - ETA: 27s - 115ms/ste - ETA: 27s - 115ms/ste - ETA: 27s - 115ms/ste - ETA: 27s - 116ms/ste - ETA: 27s - 116ms/ste - ETA: 27s - 116ms/ste - ETA: 26s - 116ms/ste - ETA: 26s - 116ms/ste - ETA: 26s - 116ms/ste - ETA: 26s - 116ms/ste - ETA: 25s - 116ms/ste - ETA: 25s - 116ms/ste - ETA: 25s - 115ms/ste - ETA: 25s - 115ms/ste - ETA: 24s - 116ms/ste - ETA: 24s - 115ms/ste - ETA: 24s - 116ms/ste - ETA: 24s - 116ms/ste - ETA: 23s - 116ms/ste - ETA: 23s - 116ms/ste - ETA: 23s - 116ms/ste - ETA: 23s - 115ms/ste - ETA: 23s - 116ms/ste - ETA: 22s - 116ms/ste - ETA: 22s - 116ms/ste - ETA: 22s - 116ms/ste - ETA: 22s - 115ms/ste - ETA: 21s - 115ms/ste - ETA: 21s - 115ms/ste - ETA: 21s - 115ms/ste - ETA: 21s - 115ms/ste - ETA: 20s - 115ms/ste - ETA: 20s - 115ms/ste - ETA: 20s - 116ms/ste - ETA: 20s - 115ms/ste - ETA: 19s - 115ms/ste - ETA: 19s - 115ms/ste - ETA: 19s - 115ms/ste - ETA: 19s - 114ms/ste - ETA: 18s - 114ms/ste - ETA: 18s - 114ms/ste - ETA: 18s - 114ms/ste - ETA: 18s - 114ms/ste - ETA: 17s - 114ms/ste - ETA: 17s - 114ms/ste - ETA: 17s - 114ms/ste - ETA: 17s - 114ms/ste - ETA: 16s - 114ms/ste - ETA: 16s - 114ms/ste - ETA: 16s - 114ms/ste - ETA: 16s - 114ms/ste - ETA: 16s - 114ms/ste - ETA: 15s - 114ms/ste - ETA: 15s - 114ms/ste - ETA: 15s - 114ms/ste - ETA: 15s - 114ms/ste - ETA: 14s - 114ms/ste - ETA: 14s - 114ms/ste - ETA: 14s - 114ms/ste - ETA: 14s - 114ms/ste - ETA: 14s - 115ms/ste - ETA: 13s - 115ms/ste - ETA: 13s - 115ms/ste - ETA: 13s - 115ms/ste - ETA: 13s - 115ms/ste - ETA: 12s - 115ms/ste - ETA: 12s - 115ms/ste - ETA: 12s - 115ms/ste - ETA: 12s - 115ms/ste - ETA: 12s - 116ms/ste - ETA: 11s - 116ms/ste - ETA: 11s - 115ms/ste - ETA: 11s - 116ms/ste - ETA: 11s - 116ms/ste - ETA: 10s - 115ms/ste - ETA: 10s - 115ms/ste - ETA: 10s - 116ms/ste - ETA: 10s - 116ms/ste - ETA: 10s - 116ms/ste - ETA: 9s - 115ms/ste - ETA: 9s - 115ms/st - ETA: 9s - 115ms/st - ETA: 9s - 116ms/st - ETA: 8s - 115ms/st - ETA: 8s - 115ms/st - ETA: 8s - 115ms/st - ETA: 8s - 115ms/st - ETA: 7s - 115ms/st - ETA: 7s - 115ms/st - ETA: 7s - 115ms/st - ETA: 7s - 115ms/st - ETA: 7s - 115ms/st - ETA: 6s - 115ms/st - ETA: 6s - 115ms/st - ETA: 6s - 115ms/st - ETA: 6s - 115ms/st - ETA: 5s - 115ms/st - ETA: 5s - 115ms/st - ETA: 5s - 115ms/st - ETA: 5s - 116ms/st - ETA: 4s - 115ms/st - ETA: 4s - 116ms/st - ETA: 4s - 116ms/st - ETA: 4s - 115ms/st - ETA: 4s - 116ms/st - ETA: 3s - 116ms/st - ETA: 3s - 116ms/st - ETA: 3s - 116ms/st - ETA: 3s - 116ms/st - ETA: 2s - 116ms/st - ETA: 2s - 116ms/st - ETA: 2s - 116ms/st - ETA: 2s - 116ms/st - ETA: 1s - 116ms/st - ETA: 1s - 117ms/st - ETA: 1s - 117ms/st - ETA: 1s - 117ms/st - ETA: 1s - 116ms/st - ETA: 0s - 116ms/st - ETA: 0s - 116ms/st - ETA: 0s - 116ms/st - ETA: 0s - 116ms/st - 116ms/step          \n",
      "Predict samples: 10000\n"
     ]
    }
   ],
   "source": [
    "import paddle.nn.functional as F\n",
    "\n",
    "\n",
    "predictions = []\n",
    "logits = model.predict(test_loader)\n",
    "\n",
    "for batch in logits[0]:\n",
    "    batch = paddle.to_tensor(batch)\n",
    "    probs = F.softmax(batch, axis=1)\n",
    "    preds = paddle.argmax(probs, axis=1).numpy().tolist()\n",
    "    predictions.extend(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-28T21:37:54.031249Z",
     "iopub.status.busy": "2022-03-28T21:37:54.030692Z",
     "iopub.status.idle": "2022-03-28T21:37:54.042042Z",
     "shell.execute_reply": "2022-03-28T21:37:54.041364Z",
     "shell.execute_reply.started": "2022-03-28T21:37:54.031186Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with open('bq_corpus.tsv', 'w') as f:\n",
    "    f.write(\"index\\tprediction\")\n",
    "    for idx, p in enumerate(predictions):\n",
    "        f.write(f\"\\n{idx}\\t{p}\")\n",
    "    f.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "py35-paddle1.2.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
