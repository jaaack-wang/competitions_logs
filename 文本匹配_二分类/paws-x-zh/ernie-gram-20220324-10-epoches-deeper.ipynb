{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-25T01:29:44.038499Z",
     "iopub.status.busy": "2022-03-25T01:29:44.038316Z",
     "iopub.status.idle": "2022-03-25T01:29:44.041761Z",
     "shell.execute_reply": "2022-03-25T01:29:44.041021Z",
     "shell.execute_reply.started": "2022-03-25T01:29:44.038471Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# !unzip /home/aistudio/data/data78992/lcqmc.zip -d /home/aistudio/data/\n",
    "# !unzip /home/aistudio/data/data78992/paws-x-zh.zip -d /home/aistudio/data/\n",
    "# !unzip /home/aistudio/data/data78992/bq_corpus.zip -d /home/aistudio/data/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-25T01:29:44.043217Z",
     "iopub.status.busy": "2022-03-25T01:29:44.042784Z",
     "iopub.status.idle": "2022-03-25T01:29:44.049233Z",
     "shell.execute_reply": "2022-03-25T01:29:44.048493Z",
     "shell.execute_reply.started": "2022-03-25T01:29:44.043192Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def load_dataset(fpath, num_row_skip=0):\n",
    "\n",
    "    def read(fp):\n",
    "        data = open(fp)\n",
    "\n",
    "        for _ in range(num_row_skip):\n",
    "            next(data)\n",
    "\n",
    "        if \"test\" in fp:\n",
    "            for line in data:\n",
    "                line = line.strip().split('\\t')\n",
    "                yield line[0], line[1]\n",
    "        else:\n",
    "            for line in data:\n",
    "                line = line.strip().split('\\t')\n",
    "                if len(line) == 3:\n",
    "                    yield line[0], line[1], int(line[2])\n",
    "\n",
    "    if isinstance(fpath, str):\n",
    "        return list(read(fpath))\n",
    "    elif isinstance(fpath, (list, tuple)):\n",
    "        return [list(read(fp)) for fp in fpath]\n",
    "    else:\n",
    "        raise TypeError(\"Input fpath must be a str or a list/tuple of str\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-25T01:29:44.050916Z",
     "iopub.status.busy": "2022-03-25T01:29:44.050440Z",
     "iopub.status.idle": "2022-03-25T01:29:44.198225Z",
     "shell.execute_reply": "2022-03-25T01:29:44.197550Z",
     "shell.execute_reply.started": "2022-03-25T01:29:44.050891Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_set, dev_set, test_set = load_dataset(['./data/paws-x-zh/train.tsv', './data/paws-x-zh/dev.tsv', './data/paws-x-zh/test.tsv'])\n",
    "# len(train_set), len(dev_set), len(test_set)\n",
    "train_set = train_set + dev_set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Transform text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-25T01:29:44.199788Z",
     "iopub.status.busy": "2022-03-25T01:29:44.199252Z",
     "iopub.status.idle": "2022-03-25T01:29:46.066444Z",
     "shell.execute_reply": "2022-03-25T01:29:46.065862Z",
     "shell.execute_reply.started": "2022-03-25T01:29:44.199757Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-03-25 09:29:46,050] [    INFO] - Already cached /home/aistudio/.paddlenlp/models/ernie-gram-zh/vocab.txt\n"
     ]
    }
   ],
   "source": [
    "from paddlenlp.datasets import MapDataset\n",
    "from paddle.io import BatchSampler, DataLoader\n",
    "from paddlenlp.data import Pad, Stack, Tuple\n",
    "from paddlenlp.transformers import ErnieGramModel as SeqClfModel\n",
    "from paddlenlp.transformers import ErnieGramTokenizer as PTMTokenizer\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "MODEL_NAME = \"ernie-gram-zh\"\n",
    "tokenizer = PTMTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "\n",
    "def example_converter(example, max_seq_length, tokenizer):\n",
    "    text_a, text_b, label = example\n",
    "    encoded = tokenizer(text=text_a, text_pair=text_b, max_seq_len=max_seq_length)\n",
    "    input_ids = encoded[\"input_ids\"]\n",
    "    token_type_ids = encoded[\"token_type_ids\"]\n",
    "    label = np.array([label], dtype=\"int64\")\n",
    "    return input_ids, token_type_ids, label\n",
    "\n",
    "\n",
    "def get_trans_fn(max_seq_length=128, tokenizer=tokenizer):\n",
    "    return lambda ex: example_converter(ex, max_seq_length, tokenizer)\n",
    "\n",
    "\n",
    "batchify_fn = lambda samples, fn=Tuple(\n",
    "    Pad(axis=0, pad_val=tokenizer.pad_token_id), \n",
    "    Pad(axis=0, pad_val=tokenizer.pad_token_type_id),\n",
    "    Stack(dtype=\"int64\")\n",
    "    ): fn(samples)\n",
    "\n",
    "\n",
    "def create_dataloader(dataset, \n",
    "                      trans_fn, \n",
    "                      batchify_fn, \n",
    "                      test=False,\n",
    "                      batch_size=128, \n",
    "                      shuffle=True, \n",
    "                      sampler=BatchSampler):\n",
    "    \n",
    "    if test:\n",
    "        dataset = [d + (0,) for d in dataset]\n",
    "\n",
    "    if not isinstance(dataset, MapDataset):\n",
    "        dataset = MapDataset(dataset)\n",
    "        \n",
    "    dataset.map(trans_fn)\n",
    "    batch_sampler = sampler(dataset, \n",
    "                            shuffle=shuffle, \n",
    "                            batch_size=batch_size)\n",
    "    \n",
    "    dataloder = DataLoader(dataset, \n",
    "                           batch_sampler=batch_sampler, \n",
    "                           collate_fn=batchify_fn)\n",
    "    \n",
    "    return dataloder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-25T01:29:46.067938Z",
     "iopub.status.busy": "2022-03-25T01:29:46.067483Z",
     "iopub.status.idle": "2022-03-25T01:29:46.073686Z",
     "shell.execute_reply": "2022-03-25T01:29:46.073140Z",
     "shell.execute_reply.started": "2022-03-25T01:29:46.067907Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "max_seq_length = 128; batch_size = 64\n",
    "trans_fn = get_trans_fn(max_seq_length)\n",
    "train_loader = create_dataloader(train_set, trans_fn, batchify_fn, batch_size=batch_size)\n",
    "# dev_loader = create_dataloader(dev_set, trans_fn, batchify_fn, batch_size=batch_size)\n",
    "test_loader = create_dataloader(test_set, trans_fn, batchify_fn, shuffle=False, test=True, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-25T01:34:48.799518Z",
     "iopub.status.busy": "2022-03-25T01:34:48.798921Z",
     "iopub.status.idle": "2022-03-25T01:34:48.807150Z",
     "shell.execute_reply": "2022-03-25T01:34:48.806386Z",
     "shell.execute_reply.started": "2022-03-25T01:34:48.799477Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from paddle import nn\n",
    "import paddle\n",
    "\n",
    "\n",
    "class PTM(nn.Layer):\n",
    "\n",
    "    def __init__(self, pretrained_model, dropout=0.1, num_class=2):\n",
    "        super().__init__()\n",
    "\n",
    "        self.ptm = pretrained_model\n",
    "        ptm_out_dim = self.ptm.config[\"hidden_size\"]\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc1 = nn.Linear(ptm_out_dim, ptm_out_dim // 2)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(ptm_out_dim // 2, num_class)\n",
    "\n",
    "    def encoder(self, input_ids, token_type_ids):\n",
    "        _, embd = self.ptm(input_ids, token_type_ids)\n",
    "        embd = self.dropout(embd)\n",
    "        return embd\n",
    "\n",
    "    def forward(self, input_ids, token_type_ids):\n",
    "        embd = self.encoder(input_ids, token_type_ids)\n",
    "        hidden = self.relu(self.fc1(embd))\n",
    "        logits = self.fc2(hidden)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-25T01:34:56.591611Z",
     "iopub.status.busy": "2022-03-25T01:34:56.590458Z",
     "iopub.status.idle": "2022-03-25T01:34:56.598191Z",
     "shell.execute_reply": "2022-03-25T01:34:56.597386Z",
     "shell.execute_reply.started": "2022-03-25T01:34:56.591571Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from paddlenlp.transformers import LinearDecayWithWarmup\n",
    "\n",
    "epoch = 10\n",
    "weight_decay = 0.01\n",
    "warmup_proportion = 0.1\n",
    "lr_scheduler = LinearDecayWithWarmup(4e-5, len(train_loader) * epoch,\n",
    "                                         warmup_proportion)\n",
    "\n",
    "def get_model(model):\n",
    "    decay_params = [\n",
    "        p.name for n, p in model.named_parameters()\n",
    "        if not any(nd in n for nd in [\"bias\", \"norm\"])\n",
    "    ]\n",
    "    optimizer = paddle.optimizer.AdamW(\n",
    "    parameters=model.parameters(), \n",
    "    learning_rate=lr_scheduler, \n",
    "    weight_decay=weight_decay, \n",
    "    apply_decay_param_fun=lambda x: x in decay_params)\n",
    "\n",
    "    criterion = paddle.nn.CrossEntropyLoss()\n",
    "\n",
    "    model = paddle.Model(model)\n",
    "    metric = paddle.metric.Accuracy()\n",
    "    model.prepare(optimizer, criterion, metric)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-25T01:34:59.951427Z",
     "iopub.status.busy": "2022-03-25T01:34:59.950276Z",
     "iopub.status.idle": "2022-03-25T01:35:04.361533Z",
     "shell.execute_reply": "2022-03-25T01:35:04.360610Z",
     "shell.execute_reply.started": "2022-03-25T01:34:59.951368Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-03-25 09:34:59,952] [    INFO] - Already cached /home/aistudio/.paddlenlp/models/ernie-gram-zh/ernie_gram_zh.pdparams\n"
     ]
    }
   ],
   "source": [
    "ptm = SeqClfModel.from_pretrained(MODEL_NAME)\n",
    "model = PTM(ptm)\n",
    "model = get_model(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-25T01:35:04.659574Z",
     "iopub.status.busy": "2022-03-25T01:35:04.658545Z",
     "iopub.status.idle": "2022-03-25T02:29:37.392443Z",
     "shell.execute_reply": "2022-03-25T02:29:37.391739Z",
     "shell.execute_reply.started": "2022-03-25T01:35:04.659536Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The loss value printed in the log is the current step, and the metric is the average value of previous steps.\n",
      "Epoch 1/10\n",
      "step 100/799 - loss: 0.6856 - acc: 0.4938 - 404ms/step\n",
      "step 200/799 - loss: 0.6622 - acc: 0.5162 - 405ms/step\n",
      "step 300/799 - loss: 0.6472 - acc: 0.5334 - 406ms/step\n",
      "step 400/799 - loss: 0.5190 - acc: 0.5644 - 406ms/step\n",
      "step 500/799 - loss: 0.4061 - acc: 0.6054 - 406ms/step\n",
      "step 600/799 - loss: 0.3992 - acc: 0.6405 - 406ms/step\n",
      "step 700/799 - loss: 0.3925 - acc: 0.6696 - 407ms/step\n",
      "step 799/799 - loss: 0.1941 - acc: 0.6923 - 407ms/step\n",
      "Epoch 2/10\n",
      "step 100/799 - loss: 0.3036 - acc: 0.8717 - 409ms/step\n",
      "step 200/799 - loss: 0.2707 - acc: 0.8784 - 408ms/step\n",
      "step 300/799 - loss: 0.2742 - acc: 0.8791 - 409ms/step\n",
      "step 400/799 - loss: 0.3198 - acc: 0.8807 - 409ms/step\n",
      "step 500/799 - loss: 0.2394 - acc: 0.8832 - 409ms/step\n",
      "step 600/799 - loss: 0.2263 - acc: 0.8847 - 409ms/step\n",
      "step 700/799 - loss: 0.1447 - acc: 0.8859 - 409ms/step\n",
      "step 799/799 - loss: 0.2232 - acc: 0.8876 - 409ms/step\n",
      "Epoch 3/10\n",
      "step 100/799 - loss: 0.2153 - acc: 0.9258 - 410ms/step\n",
      "step 200/799 - loss: 0.2290 - acc: 0.9224 - 409ms/step\n",
      "step 300/799 - loss: 0.2865 - acc: 0.9225 - 409ms/step\n",
      "step 400/799 - loss: 0.2257 - acc: 0.9219 - 409ms/step\n",
      "step 500/799 - loss: 0.0847 - acc: 0.9230 - 409ms/step\n",
      "step 600/799 - loss: 0.2666 - acc: 0.9220 - 409ms/step\n",
      "step 700/799 - loss: 0.2148 - acc: 0.9221 - 409ms/step\n",
      "step 799/799 - loss: 0.2148 - acc: 0.9222 - 409ms/step\n",
      "Epoch 4/10\n",
      "step 100/799 - loss: 0.1373 - acc: 0.9453 - 413ms/step\n",
      "step 200/799 - loss: 0.2139 - acc: 0.9441 - 410ms/step\n",
      "step 300/799 - loss: 0.1265 - acc: 0.9445 - 409ms/step\n",
      "step 400/799 - loss: 0.2645 - acc: 0.9439 - 408ms/step\n",
      "step 500/799 - loss: 0.0915 - acc: 0.9443 - 409ms/step\n",
      "step 600/799 - loss: 0.0568 - acc: 0.9441 - 409ms/step\n",
      "step 700/799 - loss: 0.1801 - acc: 0.9427 - 410ms/step\n",
      "step 799/799 - loss: 0.1396 - acc: 0.9426 - 410ms/step\n",
      "Epoch 5/10\n",
      "step 100/799 - loss: 0.1528 - acc: 0.9567 - 411ms/step\n",
      "step 200/799 - loss: 0.1507 - acc: 0.9573 - 411ms/step\n",
      "step 300/799 - loss: 0.0984 - acc: 0.9566 - 411ms/step\n",
      "step 400/799 - loss: 0.0906 - acc: 0.9560 - 410ms/step\n",
      "step 500/799 - loss: 0.1254 - acc: 0.9560 - 411ms/step\n",
      "step 600/799 - loss: 0.0514 - acc: 0.9562 - 411ms/step\n",
      "step 700/799 - loss: 0.1649 - acc: 0.9557 - 410ms/step\n",
      "step 799/799 - loss: 0.1551 - acc: 0.9560 - 410ms/step\n",
      "Epoch 6/10\n",
      "step 100/799 - loss: 0.1453 - acc: 0.9719 - 412ms/step\n",
      "step 200/799 - loss: 0.1498 - acc: 0.9709 - 411ms/step\n",
      "step 300/799 - loss: 0.0518 - acc: 0.9691 - 411ms/step\n",
      "step 400/799 - loss: 0.0573 - acc: 0.9686 - 411ms/step\n",
      "step 500/799 - loss: 0.0235 - acc: 0.9688 - 411ms/step\n",
      "step 600/799 - loss: 0.1378 - acc: 0.9677 - 410ms/step\n",
      "step 700/799 - loss: 0.0884 - acc: 0.9676 - 410ms/step\n",
      "step 799/799 - loss: 0.0606 - acc: 0.9679 - 410ms/step\n",
      "Epoch 7/10\n",
      "step 100/799 - loss: 0.1129 - acc: 0.9789 - 413ms/step\n",
      "step 200/799 - loss: 0.0506 - acc: 0.9797 - 414ms/step\n",
      "step 300/799 - loss: 0.0439 - acc: 0.9784 - 414ms/step\n",
      "step 400/799 - loss: 0.1040 - acc: 0.9764 - 413ms/step\n",
      "step 500/799 - loss: 0.0157 - acc: 0.9765 - 413ms/step\n",
      "step 600/799 - loss: 0.0823 - acc: 0.9763 - 412ms/step\n",
      "step 700/799 - loss: 0.0282 - acc: 0.9765 - 411ms/step\n",
      "step 799/799 - loss: 0.0232 - acc: 0.9763 - 411ms/step\n",
      "Epoch 8/10\n",
      "step 100/799 - loss: 0.0677 - acc: 0.9812 - 409ms/step\n",
      "step 200/799 - loss: 0.0280 - acc: 0.9807 - 409ms/step\n",
      "step 300/799 - loss: 0.0588 - acc: 0.9805 - 409ms/step\n",
      "step 400/799 - loss: 0.0111 - acc: 0.9807 - 409ms/step\n",
      "step 500/799 - loss: 0.2304 - acc: 0.9808 - 410ms/step\n",
      "step 600/799 - loss: 0.0185 - acc: 0.9808 - 410ms/step\n",
      "step 700/799 - loss: 0.0164 - acc: 0.9809 - 410ms/step\n",
      "step 799/799 - loss: 0.1099 - acc: 0.9807 - 410ms/step\n",
      "Epoch 9/10\n",
      "step 100/799 - loss: 0.0628 - acc: 0.9836 - 409ms/step\n",
      "step 200/799 - loss: 0.0191 - acc: 0.9843 - 411ms/step\n",
      "step 300/799 - loss: 0.0444 - acc: 0.9846 - 411ms/step\n",
      "step 400/799 - loss: 0.0327 - acc: 0.9850 - 410ms/step\n",
      "step 500/799 - loss: 0.0866 - acc: 0.9850 - 410ms/step\n",
      "step 600/799 - loss: 0.0031 - acc: 0.9851 - 410ms/step\n",
      "step 700/799 - loss: 0.0105 - acc: 0.9851 - 410ms/step\n",
      "step 799/799 - loss: 0.0100 - acc: 0.9854 - 409ms/step\n",
      "Epoch 10/10\n",
      "step 100/799 - loss: 0.0059 - acc: 0.9883 - 414ms/step\n",
      "step 200/799 - loss: 0.0033 - acc: 0.9875 - 413ms/step\n",
      "step 300/799 - loss: 0.0390 - acc: 0.9884 - 412ms/step\n",
      "step 400/799 - loss: 0.0152 - acc: 0.9885 - 411ms/step\n",
      "step 500/799 - loss: 0.0192 - acc: 0.9886 - 411ms/step\n",
      "step 600/799 - loss: 0.0136 - acc: 0.9888 - 411ms/step\n",
      "step 700/799 - loss: 0.0204 - acc: 0.9888 - 411ms/step\n",
      "step 799/799 - loss: 0.0065 - acc: 0.9889 - 410ms/step\n"
     ]
    }
   ],
   "source": [
    "model.fit(train_loader, epochs=epoch, verbose=2, log_freq=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-25T02:29:37.414812Z",
     "iopub.status.busy": "2022-03-25T02:29:37.414649Z",
     "iopub.status.idle": "2022-03-25T02:29:42.775344Z",
     "shell.execute_reply": "2022-03-25T02:29:42.774703Z",
     "shell.execute_reply.started": "2022-03-25T02:29:37.414792Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predict begin...\n",
      "step 32/32 [==============================] - ETA: 7s - 234ms/st - ETA: 5s - 202ms/st - ETA: 4s - 192ms/st - ETA: 4s - 187ms/st - ETA: 4s - 184ms/st - ETA: 3s - 181ms/st - ETA: 3s - 179ms/st - ETA: 2s - 178ms/st - ETA: 2s - 179ms/st - ETA: 2s - 177ms/st - ETA: 1s - 177ms/st - ETA: 1s - 176ms/st - ETA: 1s - 176ms/st - ETA: 0s - 175ms/st - ETA: 0s - 172ms/st - 167ms/step          \n",
      "Predict samples: 2000\n"
     ]
    }
   ],
   "source": [
    "import paddle.nn.functional as F\n",
    "\n",
    "\n",
    "predictions = []\n",
    "logits = model.predict(test_loader)\n",
    "\n",
    "for batch in logits[0]:\n",
    "    batch = paddle.to_tensor(batch)\n",
    "    probs = F.softmax(batch, axis=1)\n",
    "    preds = paddle.argmax(probs, axis=1).numpy().tolist()\n",
    "    predictions.extend(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-25T02:29:42.776909Z",
     "iopub.status.busy": "2022-03-25T02:29:42.776438Z",
     "iopub.status.idle": "2022-03-25T02:29:42.781955Z",
     "shell.execute_reply": "2022-03-25T02:29:42.781212Z",
     "shell.execute_reply.started": "2022-03-25T02:29:42.776880Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with open('paws-x.tsv', 'w') as f:\n",
    "    f.write(\"index\\tprediction\")\n",
    "    for idx, p in enumerate(predictions):\n",
    "        f.write(f\"\\n{idx}\\t{p}\")\n",
    "    f.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "py35-paddle1.2.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
